{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Vulture tutorial The content of this tutorial is derived from a tutorial Nextflow with Batch . We changed the goal to run our Vulture pipeline, which we will run locally in containers and submit locally to AWS Batch. Overview In this tutorial, you will get hands-on experience with Vulture pipeline. The goal of this tutorial is to get familiar with using Vulture and its pipeline architecture based on the following AWS services. AWS EC2: cloud server instances AWS EC2 Spot Instances: EC2 instances benefit from unused EC2 capacity AWS Batch: Job scheduler of containerized workflow in batch AWS S3: Object storage Other concepts Docker: container services Nextflow: language reproducible scientific workflows","title":"Welcome"},{"location":"#welcome-to-the-vulture-tutorial","text":"The content of this tutorial is derived from a tutorial Nextflow with Batch . We changed the goal to run our Vulture pipeline, which we will run locally in containers and submit locally to AWS Batch.","title":"Welcome to the Vulture tutorial"},{"location":"#overview","text":"In this tutorial, you will get hands-on experience with Vulture pipeline. The goal of this tutorial is to get familiar with using Vulture and its pipeline architecture based on the following AWS services. AWS EC2: cloud server instances AWS EC2 Spot Instances: EC2 instances benefit from unused EC2 capacity AWS Batch: Job scheduler of containerized workflow in batch AWS S3: Object storage","title":"Overview"},{"location":"#other-concepts","text":"Docker: container services Nextflow: language reproducible scientific workflows","title":"Other concepts"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/","text":"Quick start PLACEHOLDER","title":"Quick start"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#quick-start","text":"PLACEHOLDER","title":"Quick start"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/","text":"Create Batch Compute Environmnet Requirements Input data: 10x Chromium scRNA-seq reads DropletUtils >= v1.10.2 STAR >= v2.7.9a or cellranger >= 6.0.0 or Kallisto/bustools >= 0.25.1 or salmon/alevin >= v1.4.0 General usage Map 10x scRNA-seq reads to the viral (and microbial) host reference set using STARsolo, CellRanger, Kallisto|bustools, or Salmon|Alevin. 0. Prerequiresits to download genome files You need to download virus genome, prokaryotes genome, combined genome and virus combined genome in the following link and save them in a folder as \"vmh_genome_dir\" to be used in the next step. human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.removed_amb_viral_exon.gtf human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.fa human_host_viruses.viruSITE.with_hg38.removed_amb_viral_exon.gtf human_host_viruses.viruSITE.with_hg38.fa 1. Map 10x scRNA-seq reads to the viral microbial host reference set: Usage: scvh_map_reads.pl [Options] <vmh_genome_dir> <R2> <R1> or <vmh_genome_dir> <.bam file> Options: Defaults -o/--output-dir <string> the output directory [./] -t/--threads <int> number of threads to run alignment with [<1>] -d/--database <string> select virus or virus and prokaryotes database, can be 'viruSITE' or 'viruSITE.NCBIprokaryotes' [<viruSITE.NCBIprokaryotes>] -e/--exe <string> executable command or stand alone executable path of the alignment tool [<>] -s/--soloStrand <string> STARsolo param: Reverse or Forward used for 10x 5' or 3' protocol, respectively [<Reverse>] -w/--whitelist <string> STARsolo param --soloCBwhitelist [<\"vmh_genome_dir\"/737K-august-2016.txt>] -r/--ram <int> limitation of RAM usage. For STARsolo, param: limitGenomeGenerateRAM, limitBAMsortRAM unit by GB [<128>] -f/--soloFeature <string> STARsolo param: See --soloFeatures in STARsolo manual [<Gene>] -ot/--outSAMtype <string> STARsolo param: See --outSAMtype in STARsolo manual [<BAM SortedByCoordinate>] -mm/--soloMultiMappers <string> STARsolo param: See --soloMultiMappers in STARsolo manual [<EM>] -a/--alignment <string> Select alignment methods: 'STAR', 'KB', 'Alevin', or 'CellRanger' [<STAR>] -v/--technology <string> KB param: Single-cell technology used (`kb --list` to view) [<10XV2>] --soloCBstart <string> STARsolo param: See --soloCBstart in STARsolo manual [<1>] --soloCBlen <string> STARsolo param: See --soloCBlen in STARsolo manual [<16>] --soloUMIstart <string> STARsolo param: See --soloUMIstart in STARsolo manual [<17>] --soloUMIlen <string> STARsolo param: See --soloUMIlen in STARsolo manual [<10>] --soloInputSAMattrBarcodeSeq <string> STARsolo param: See --soloInputSAMattrBarcodeSeq in STARsolo manual [<CR UR>] For fastqs alignment option 'STAR', 'KB', and 'Alevin', run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir R2.fastq.gz R1.fastq.gz where -t is a user-specified integer indicating number of threads to run with, output_dir is a user-specified directory to place the outputs, vmh_genome_dir is a pre-generated viral (and microbial) host (human) reference set directory, R2.fastq.gz and R1.fastq.gz are input 10x scRNA-seq reads. For fastqs option 'CellRanger', run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir sample fastqs where sample and fastqs are two cellranger arguments: --sample and --fastqs . See documentation in cellranger count to infer rules of fastq and sample naming. For bam files, we only support STARsolo, run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir your_bam_file.bam 2. Filter the mapped UMIs using EmptyDrops to get the viral (and microbial) host filtered UMI counts matrix and also output viral genes and barcodes info files: Usage: Rscript scvh_filter_matrix.r output_dir sample_name where sample_name is an optional user-specified tag to be used as a prefix for the output files. 3. (Optional, and STARsolo or CellRanger only) Output some quality control criteria of the post-EmptyDrops viral microbial supporting reads in the BAM file Usage: perl scvh_analyze_bam.pl output_dir sample_name [Previous Step](https://juychen.github.io/docs/6_Local/Quickstart.html){: .btn } [Next Step](https://juychen.github.io/docs/6_Local/Nextflow.html){: .btn .btn-purple }","title":"Command line"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#create-batch-compute-environmnet","text":"","title":"Create Batch Compute Environmnet"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#requirements","text":"Input data: 10x Chromium scRNA-seq reads DropletUtils >= v1.10.2 STAR >= v2.7.9a or cellranger >= 6.0.0 or Kallisto/bustools >= 0.25.1 or salmon/alevin >= v1.4.0","title":"Requirements"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#general-usage","text":"Map 10x scRNA-seq reads to the viral (and microbial) host reference set using STARsolo, CellRanger, Kallisto|bustools, or Salmon|Alevin.","title":"General usage"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#0-prerequiresits-to-download-genome-files","text":"You need to download virus genome, prokaryotes genome, combined genome and virus combined genome in the following link and save them in a folder as \"vmh_genome_dir\" to be used in the next step. human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.removed_amb_viral_exon.gtf human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.fa human_host_viruses.viruSITE.with_hg38.removed_amb_viral_exon.gtf human_host_viruses.viruSITE.with_hg38.fa","title":"0. Prerequiresits to download genome files"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#1-map-10x-scrna-seq-reads-to-the-viral-microbial-host-reference-set","text":"Usage: scvh_map_reads.pl [Options] <vmh_genome_dir> <R2> <R1> or <vmh_genome_dir> <.bam file> Options: Defaults -o/--output-dir <string> the output directory [./] -t/--threads <int> number of threads to run alignment with [<1>] -d/--database <string> select virus or virus and prokaryotes database, can be 'viruSITE' or 'viruSITE.NCBIprokaryotes' [<viruSITE.NCBIprokaryotes>] -e/--exe <string> executable command or stand alone executable path of the alignment tool [<>] -s/--soloStrand <string> STARsolo param: Reverse or Forward used for 10x 5' or 3' protocol, respectively [<Reverse>] -w/--whitelist <string> STARsolo param --soloCBwhitelist [<\"vmh_genome_dir\"/737K-august-2016.txt>] -r/--ram <int> limitation of RAM usage. For STARsolo, param: limitGenomeGenerateRAM, limitBAMsortRAM unit by GB [<128>] -f/--soloFeature <string> STARsolo param: See --soloFeatures in STARsolo manual [<Gene>] -ot/--outSAMtype <string> STARsolo param: See --outSAMtype in STARsolo manual [<BAM SortedByCoordinate>] -mm/--soloMultiMappers <string> STARsolo param: See --soloMultiMappers in STARsolo manual [<EM>] -a/--alignment <string> Select alignment methods: 'STAR', 'KB', 'Alevin', or 'CellRanger' [<STAR>] -v/--technology <string> KB param: Single-cell technology used (`kb --list` to view) [<10XV2>] --soloCBstart <string> STARsolo param: See --soloCBstart in STARsolo manual [<1>] --soloCBlen <string> STARsolo param: See --soloCBlen in STARsolo manual [<16>] --soloUMIstart <string> STARsolo param: See --soloUMIstart in STARsolo manual [<17>] --soloUMIlen <string> STARsolo param: See --soloUMIlen in STARsolo manual [<10>] --soloInputSAMattrBarcodeSeq <string> STARsolo param: See --soloInputSAMattrBarcodeSeq in STARsolo manual [<CR UR>] For fastqs alignment option 'STAR', 'KB', and 'Alevin', run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir R2.fastq.gz R1.fastq.gz where -t is a user-specified integer indicating number of threads to run with, output_dir is a user-specified directory to place the outputs, vmh_genome_dir is a pre-generated viral (and microbial) host (human) reference set directory, R2.fastq.gz and R1.fastq.gz are input 10x scRNA-seq reads. For fastqs option 'CellRanger', run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir sample fastqs where sample and fastqs are two cellranger arguments: --sample and --fastqs . See documentation in cellranger count to infer rules of fastq and sample naming. For bam files, we only support STARsolo, run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir your_bam_file.bam","title":"1. Map 10x scRNA-seq reads to the viral microbial host reference set:"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#2-filter-the-mapped-umis-using-emptydrops-to-get-the-viral-and-microbial-host-filtered-umi-counts-matrix-and-also-output-viral-genes-and-barcodes-info-files","text":"Usage: Rscript scvh_filter_matrix.r output_dir sample_name where sample_name is an optional user-specified tag to be used as a prefix for the output files.","title":"2. Filter the mapped UMIs using EmptyDrops to get the viral (and microbial) host filtered UMI counts matrix and also output viral genes and barcodes info files:"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#3-optional-and-starsolo-or-cellranger-only-output-some-quality-control-criteria-of-the-post-emptydrops-viral-microbial-supporting-reads-in-the-bam-file","text":"Usage: perl scvh_analyze_bam.pl output_dir sample_name [Previous Step](https://juychen.github.io/docs/6_Local/Quickstart.html){: .btn } [Next Step](https://juychen.github.io/docs/6_Local/Nextflow.html){: .btn .btn-purple }","title":"3. (Optional, and STARsolo or CellRanger only) Output some quality control criteria of the post-EmptyDrops viral microbial supporting reads in the BAM file"},{"location":"1.%20Running%20on%20local%20computer/3_Nextflow/","text":"Create Nextflow environment PLACEHOLDER","title":"Nextflow"},{"location":"1.%20Running%20on%20local%20computer/3_Nextflow/#create-nextflow-environment","text":"PLACEHOLDER","title":"Create Nextflow environment"},{"location":"1.%20Running%20on%20local%20computer/4_Docker/","text":"Download Docker image PLACEHOLDER","title":"Docker"},{"location":"1.%20Running%20on%20local%20computer/4_Docker/#download-docker-image","text":"PLACEHOLDER","title":"Download Docker image"},{"location":"2.%20Running%20on%20AWS%20Cloud/1_Setup/","text":"To get started with Vulture, you will need a AWS account and setup the environment in your account. Vulture is composed of a stack of AWS services as below: Setup This chapter guides you to configure the following AWS services which you will use in Vulture. - AWS IAM - AWS Batch - AWS S3 Create an IAM role for your Workspace Head over to the IAM console , find and click \"Create role\" button (2) under the Roles (1) section. Select the \"AWS service\" (3) and choose the \"Batch\" use case (4) hit Next button (5) at the bottom. Select the following policies and click \"attach policies\" to add them (6) . AdministratorAccess AmazonEC2FullAccess AmazonEC2ContainerRegistryFullAccess AmazonS3FullAccess AWSBatchServiceRole AdministratorAccess AWSBatchFullAccess AmazonEC2ContainerServiceforEC2Role AmazonEC2ContainerServiceRole Name the role as \"vulture-iam-role\" and click \"Create role\". Therefore, a role named \"aws-workshop-admin\" is ready for use.","title":"Setup"},{"location":"2.%20Running%20on%20AWS%20Cloud/1_Setup/#setup","text":"This chapter guides you to configure the following AWS services which you will use in Vulture. - AWS IAM - AWS Batch - AWS S3","title":"Setup"},{"location":"2.%20Running%20on%20AWS%20Cloud/1_Setup/#create-an-iam-role-for-your-workspace","text":"Head over to the IAM console , find and click \"Create role\" button (2) under the Roles (1) section. Select the \"AWS service\" (3) and choose the \"Batch\" use case (4) hit Next button (5) at the bottom. Select the following policies and click \"attach policies\" to add them (6) . AdministratorAccess AmazonEC2FullAccess AmazonEC2ContainerRegistryFullAccess AmazonS3FullAccess AWSBatchServiceRole AdministratorAccess AWSBatchFullAccess AmazonEC2ContainerServiceforEC2Role AmazonEC2ContainerServiceRole Name the role as \"vulture-iam-role\" and click \"Create role\". Therefore, a role named \"aws-workshop-admin\" is ready for use.","title":"Create an IAM role for your Workspace"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/","text":"Overview Nextflow Nextflow is free open-source software distributed under the Apache 2.0 license developed by Seqera Labs. The software is used by scientists and engineers to write, deploy and share data-intensive, highly scalable, workflows on any infrastructure. It helps us to define workflows to run the Vulture pipeline composed of multiple programs. The reason we build Vulture within Nextflow is that it can be applied to workflows in containers or on a cloud computing platform. You can refer to the Nextflow documentation for more information about Nextflow syntax and usage. Nextflow This section introduces the installation of Nextflow and its dependencies. Install Nextflow Nextflow can be installed by following the instructions at Nextflow Installation Guide . If you are using OS X/Linux, you can run the commands below to install it immediately. wget -qO- https://get.nextflow.io | bash # run from current directory chmod +x nextflow # run from anywhere sudo mv nextflow /usr/local/bin/ Install Nextflow dependencies for visualization Nextflow is able to render graphs for which it needs graphviz to be installed. jq will help us deal with JSON files. sudo yum install -y graphviz jq AWS Region Even though we are depending on an IAM Role and not local permissions some tools depend on having the AWS_REGION defined as an environment variable - let's add it to our login shell configuration. export AWS_REGION=us-east-2 echo \"AWS_REGION=${AWS_REGION}\" |tee -a ~/.bashrc [Previous Step](https://juychen.github.io/docs/2_Setup/Setup.html){: .btn } [Next Step](https://juychen.github.io/docs/3_Nextflow/NextflowInstall.html){: .btn .btn-purple }","title":"Nextflow"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#overview","text":"","title":"Overview"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#nextflow","text":"Nextflow is free open-source software distributed under the Apache 2.0 license developed by Seqera Labs. The software is used by scientists and engineers to write, deploy and share data-intensive, highly scalable, workflows on any infrastructure. It helps us to define workflows to run the Vulture pipeline composed of multiple programs. The reason we build Vulture within Nextflow is that it can be applied to workflows in containers or on a cloud computing platform. You can refer to the Nextflow documentation for more information about Nextflow syntax and usage.","title":"Nextflow"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#nextflow_1","text":"This section introduces the installation of Nextflow and its dependencies.","title":"Nextflow"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#install-nextflow","text":"Nextflow can be installed by following the instructions at Nextflow Installation Guide . If you are using OS X/Linux, you can run the commands below to install it immediately. wget -qO- https://get.nextflow.io | bash # run from current directory chmod +x nextflow # run from anywhere sudo mv nextflow /usr/local/bin/","title":"Install Nextflow"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#install-nextflow-dependencies-for-visualization","text":"Nextflow is able to render graphs for which it needs graphviz to be installed. jq will help us deal with JSON files. sudo yum install -y graphviz jq","title":"Install Nextflow dependencies for visualization"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#aws-region","text":"Even though we are depending on an IAM Role and not local permissions some tools depend on having the AWS_REGION defined as an environment variable - let's add it to our login shell configuration. export AWS_REGION=us-east-2 echo \"AWS_REGION=${AWS_REGION}\" |tee -a ~/.bashrc [Previous Step](https://juychen.github.io/docs/2_Setup/Setup.html){: .btn } [Next Step](https://juychen.github.io/docs/3_Nextflow/NextflowInstall.html){: .btn .btn-purple }","title":"AWS Region"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/","text":"Overview AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of computational resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. With AWS Batch, there is no need to install and manage batch computing software or server clusters that you use to run your jobs, allowing you to focus on analyzing results and solving problems. AWS Batch plans, schedules, and executes your batch computing workloads across the full range of AWS compute services and features, such as AWS Fargate, Amazon EC2, and Spot Instances. There is no additional charge for AWS Batch. You only pay for the AWS resources (e.g. EC2 instances) you create to store and run your batch jobs. In our tutorial, the pipeline of Nextflow and Batch deal with jobs by the following workflow automatically\uff1a We configure and run Nextflow script to trigger a submission to the job queue. The AWS Batch scheduler runs periodically to check for jobs in the job queue Once jobs are placed, the scheduler evaluates the Compute Environments and adjusts the compute capacity to allow the jobs to run. The job (packaged by the container) will run in the computational environment created. The job output will be transferred and stored in the S3 bucket. Batch Enviroment 0. Create a launch template Before you create an AWS Batch Compute Environment, please kindly refer to how to create Vulture launch template to create a launch template first. 1. Setup Batch Vulture Compute Environment 1 Create an AWS Batch Environment at AWS Batch Home . On the AWS Batch dashboard: 1) Select \"Compute environments\" on the left panel 2) Select \"Create\" to create a new compute environments In the Compute Environments page, set your preference of the compute environment as follows: Step 1 Compute environment configuration 3) Select \"Managed\" in the compute environment orchestration type 4) Change the Name in the compute environment name. We suggest that you should use \"scvh-CE-r5a4x\". 5) Select \"vulture-iam-role\" in the Service role ( IAM role settings ). 6) Select \"ecsInstanceRole\" in the Instance role. Step 2 Instance configuration 7) Select \"Use EC2 Spot Instances\" for cost-saving 8) Set \"0\" in both Minium and Desired vCPU settings, and 256 in Maximum CPUs -required 9) Scoll down to the \"Additional configuration\". 10) Select \"optimal\" in the Allowed instance type. 11) Select \"SPOT_CAPACITY_OPTIMIZED\" in the allocation strategy 12) Select \"vulture-launch-template\" under Launch template. For how to create this template beforehand. Please refer to launch template . We apply this template to every instance Batch schedules for us because we have mentioned in the previous section that the default storage of EC2 instance is not enough for the Vulture pipeline, this template will provide extra spaces for our Batch job's instance. Step 3 Network configuration 13) Select \"Default for VPC\" under the Virtual Private Cloud (VPC) ID, this will select the default VPCs . Note that all AWS accounts comes with a default VPC for use in each Region. A default VPC comes with a public subnet in each Availability Zone, an internet gateway, and settings to enable DNS resolution. Therefore, you can immediately start launching Amazon EC2 instances into a default VPC. A default VPC is suitable for getting started quickly. Leave other settings of the environment as default and create an environment. 2. Setup Batch Vulture Compute Environment 2 This basically follows the same steps as to create Compute Environment 1 above, but with different name and instance type, to enable massively parallel processing of Vulture tasks with Batch. 1) Select \"Compute environments\" on the left panel 2) Select \"Create\" to create a new compute environments Repeat the configuration steps from Step 2. to Step 13 to finish create Compute Environment 2 used for creating Compute Environment 1 above. Monitoring Batch Job Heading back to the Batch Home dashboard , you can see the overview of the number of tasks running in the job queue. To monitor your running jobs, you can click the number below the label \"RUNNING\". Here, you can find the job submited previously. Click the job name to go to the job information page. Inside the information page, you can click the link below the \"Log stream name\" label. This is where the output of the program stored. It will record any information output within the stand output stream of the console. Also, the console output will refresh continuously along with your program process. 0. View results You can select go the S3 Bucket . Looking into the path: \"s3://${BUCKET_NAME_RESULTS}/batchD/{SAMPLE_ID}/alignment_outs/\" page. Click to open it. The Vulture results should look like in the picture below: 1. View pricing of the program To check the bill of our previous run, we can go to the EC2 Dashboard and find the Spot Request section at https://console.aws.amazon.com/ec2sp/v2/home#/spot . Click saving summary to view the cost we saved by applying spot instances. This saving summary page shows the price. We know that to align a 12GB scRNA-Seq sample takes $0.17. Thanks to the spot instances, we can save 79% of money compared to hosting an on-demand EC2 instance. Batch Queue 0. Setup Batch Vulture Job Queue 1 We can return back to the home page of Batch at: https://us-east-2.console.aws.amazon.com/batch/home . 1) Select \"Job queues\" on the left panel 2) Select \"Create\" to create a new job queue 3) Name your job queue to according to your preference. 4) Select your compute environment to which you created in the previous chapter i.e. Batch Environment 5) Select \"Create\" to create a new job queue 1. Setup Batch Vulture Job Queue 2 We can return back to the home page of Batch . Create a Job Queue 2 for parallel processing of Vulture tasks by repeating the same steps from Step 1 to Step 4 used to create Job Queue 1 above. [Previous Step](https://juychen.github.io/docs/5_Cloud/Nextflow.html){: .btn } [Next Step](https://juychen.github.io/docs/6_Local/Localrun.html){: .btn .btn-purple }","title":"Batch"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#overview","text":"AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of computational resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. With AWS Batch, there is no need to install and manage batch computing software or server clusters that you use to run your jobs, allowing you to focus on analyzing results and solving problems. AWS Batch plans, schedules, and executes your batch computing workloads across the full range of AWS compute services and features, such as AWS Fargate, Amazon EC2, and Spot Instances. There is no additional charge for AWS Batch. You only pay for the AWS resources (e.g. EC2 instances) you create to store and run your batch jobs. In our tutorial, the pipeline of Nextflow and Batch deal with jobs by the following workflow automatically\uff1a We configure and run Nextflow script to trigger a submission to the job queue. The AWS Batch scheduler runs periodically to check for jobs in the job queue Once jobs are placed, the scheduler evaluates the Compute Environments and adjusts the compute capacity to allow the jobs to run. The job (packaged by the container) will run in the computational environment created. The job output will be transferred and stored in the S3 bucket.","title":"Overview"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#batch-enviroment","text":"","title":"Batch Enviroment"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#0-create-a-launch-template","text":"Before you create an AWS Batch Compute Environment, please kindly refer to how to create Vulture launch template to create a launch template first.","title":"0. Create a launch template"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#1-setup-batch-vulture-compute-environment-1","text":"Create an AWS Batch Environment at AWS Batch Home . On the AWS Batch dashboard: 1) Select \"Compute environments\" on the left panel 2) Select \"Create\" to create a new compute environments In the Compute Environments page, set your preference of the compute environment as follows: Step 1 Compute environment configuration 3) Select \"Managed\" in the compute environment orchestration type 4) Change the Name in the compute environment name. We suggest that you should use \"scvh-CE-r5a4x\". 5) Select \"vulture-iam-role\" in the Service role ( IAM role settings ). 6) Select \"ecsInstanceRole\" in the Instance role. Step 2 Instance configuration 7) Select \"Use EC2 Spot Instances\" for cost-saving 8) Set \"0\" in both Minium and Desired vCPU settings, and 256 in Maximum CPUs -required 9) Scoll down to the \"Additional configuration\". 10) Select \"optimal\" in the Allowed instance type. 11) Select \"SPOT_CAPACITY_OPTIMIZED\" in the allocation strategy 12) Select \"vulture-launch-template\" under Launch template. For how to create this template beforehand. Please refer to launch template . We apply this template to every instance Batch schedules for us because we have mentioned in the previous section that the default storage of EC2 instance is not enough for the Vulture pipeline, this template will provide extra spaces for our Batch job's instance. Step 3 Network configuration 13) Select \"Default for VPC\" under the Virtual Private Cloud (VPC) ID, this will select the default VPCs . Note that all AWS accounts comes with a default VPC for use in each Region. A default VPC comes with a public subnet in each Availability Zone, an internet gateway, and settings to enable DNS resolution. Therefore, you can immediately start launching Amazon EC2 instances into a default VPC. A default VPC is suitable for getting started quickly. Leave other settings of the environment as default and create an environment.","title":"1. Setup Batch Vulture Compute Environment 1"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#2-setup-batch-vulture-compute-environment-2","text":"This basically follows the same steps as to create Compute Environment 1 above, but with different name and instance type, to enable massively parallel processing of Vulture tasks with Batch. 1) Select \"Compute environments\" on the left panel 2) Select \"Create\" to create a new compute environments Repeat the configuration steps from Step 2. to Step 13 to finish create Compute Environment 2 used for creating Compute Environment 1 above.","title":"2. Setup Batch Vulture Compute Environment 2"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#monitoring-batch-job","text":"Heading back to the Batch Home dashboard , you can see the overview of the number of tasks running in the job queue. To monitor your running jobs, you can click the number below the label \"RUNNING\". Here, you can find the job submited previously. Click the job name to go to the job information page. Inside the information page, you can click the link below the \"Log stream name\" label. This is where the output of the program stored. It will record any information output within the stand output stream of the console. Also, the console output will refresh continuously along with your program process.","title":"Monitoring Batch Job"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#0-view-results","text":"You can select go the S3 Bucket . Looking into the path: \"s3://${BUCKET_NAME_RESULTS}/batchD/{SAMPLE_ID}/alignment_outs/\" page. Click to open it. The Vulture results should look like in the picture below:","title":"0. View results"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#1-view-pricing-of-the-program","text":"To check the bill of our previous run, we can go to the EC2 Dashboard and find the Spot Request section at https://console.aws.amazon.com/ec2sp/v2/home#/spot . Click saving summary to view the cost we saved by applying spot instances. This saving summary page shows the price. We know that to align a 12GB scRNA-Seq sample takes $0.17. Thanks to the spot instances, we can save 79% of money compared to hosting an on-demand EC2 instance.","title":"1. View pricing of the program"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#batch-queue","text":"","title":"Batch Queue"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#0-setup-batch-vulture-job-queue-1","text":"We can return back to the home page of Batch at: https://us-east-2.console.aws.amazon.com/batch/home . 1) Select \"Job queues\" on the left panel 2) Select \"Create\" to create a new job queue 3) Name your job queue to according to your preference. 4) Select your compute environment to which you created in the previous chapter i.e. Batch Environment 5) Select \"Create\" to create a new job queue","title":"0. Setup Batch Vulture Job Queue 1"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#1-setup-batch-vulture-job-queue-2","text":"We can return back to the home page of Batch . Create a Job Queue 2 for parallel processing of Vulture tasks by repeating the same steps from Step 1 to Step 4 used to create Job Queue 1 above. [Previous Step](https://juychen.github.io/docs/5_Cloud/Nextflow.html){: .btn } [Next Step](https://juychen.github.io/docs/6_Local/Localrun.html){: .btn .btn-purple }","title":"1. Setup Batch Vulture Job Queue 2"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/","text":"Nextflow and scRNA-Seq processing Setup the AWS CLI Install the AWS CLI and prepare the access key and secret access key ahead by following instructions at Obtaining AWS Credentials . pip install awscli aws configure # here will prompt you to enter access key and secret access key Clone Vulture source code Clone the source code of Vulture into your local computer git clone https://github.com/holab-hku/Vulture.git # change into directory below cd Vulture/nextflow # checkout the branch below for most updated code git checkout cloud-new-junyi Create S3 Bucket to store results # specify bucket names and save them into bash environment variables export BUCKET_NAME_TEMP=vulture-temp export BUCKET_NAME_RESULTS=vulture-results echo \"BUCKET_NAME_TEMP=${BUCKET_NAME_TEMP}\" |tee -a ~/.bashrc echo \"BUCKET_NAME_RESULTS=${BUCKET_NAME_RESULTS}\" |tee -a ~/.bashrc # create S3 buckets with specified bucket names aws --region ${AWS_REGION} s3 mb s3://${BUCKET_NAME_TEMP} aws --region ${AWS_REGION} s3 mb s3://${BUCKET_NAME_RESULTS} Run Vulture pipeline - 1. Build Genome reference Now we are about to run the first step of the Vulture pipeline i.e. mkref (genome reference making), execute the command below in your favourite terminal or powershell and wait it to be finished nextflow run scvh_mkref.nf -profile mkref -bucket-dir s3://${BUCKET_NAME_TEMP} --outdir=s3://${BUCKET_NAME_RESULTS}/batchA -with-report mkref_$(date +%s).html -bg &>> mkref_$(date +%s).log; The input data required for mkref stage are available in the downloadable links below, you can save them into your own S3 bucket folder: hg38.fa hg38.unique_gene_names.gtf prokaryotes.csv viruSITE_human_host.txt Also you can generate the files yourself following instructions below: VirusSITE (viruSITE human host) Click \"Format: CSV\" NCBI Prokaryotes Filters -> Host (Homo sapiens) -> Assembly level (Complete) -> RefSeq category (representative) -> Download [prokaryotes.csv] After the mkref job is done, you need to edit line in \"nextflow/nextflow.config\" file -> \"params.ref\" to the actual S3 path where your output reference genome files are i.e. in \"s3://${BUCKET_NAME_RESULTS}/batchA\" or you could download from the downloadable links below and store them into your own S3 bucket folder: human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.removed_amb_viral_exon.gtf human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.fa human_host_viruses.viruSITE.with_hg38.removed_amb_viral_exon.gtf human_host_viruses.viruSITE.with_hg38.fa ... mkref { aws.region = 'us-east-2' process.container = 'public.ecr.aws/b6a4h2a6/scvh_mkref:latest' process.executor = 'awsbatch' process.queue = 'vulture-stdq' # this line needs to be changed params.ref = 's3://vulture-reference/humangenome/' } ... Run Vulture pipeline - 2. Start main analysis Before we start our analysis, we need to edit \"nextflow/params.yaml\" file to include the reads of your interest. Here is a snippet of how the \"params.yaml\" file looks like ... soloStrand: \"Forward\" alignment: \"STAR\" technology: \"10XV2\" virus_database: \"viruSITE\" soloMultiMappers: \"EM\" soloFeatures: \"GeneFull\" inputformat: \"bam\" soloInputSAMattrBarcodeSeq: \"CB UB\" barcodes_whitelist: \"None\" reads: - \"SRR6885502\" - \"SRR6885503\" - \"SRR6885504\" - \"SRR6885505\" - \"SRR6885506\" - \"SRR6885507\" - \"SRR6885508\" ... Execute the command below to start the main analysis of Vulture. nextflow run scvh_full.nf -profile batchfull -params-file params.yaml -bucket-dir s3://${BUCKET_NAME_TEMP} --outdir=s3://${BUCKET_NAME_RESULTS}/batchD -with-report report_bam_$(date +%s).html -bg &>> submitnf_bam_$(date +%s).log [Previous Step](https://juychen.github.io/docs/3_Nextflow/NextflowInstall.html){: .btn } [Next Step](https://juychen.github.io/docs/6_Local/Localusage.html){: .btn .btn-purple }","title":"Running Vulture on AWS cloud with Nextflow"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#nextflow-and-scrna-seq-processing","text":"","title":"Nextflow and scRNA-Seq processing"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#setup-the-aws-cli","text":"Install the AWS CLI and prepare the access key and secret access key ahead by following instructions at Obtaining AWS Credentials . pip install awscli aws configure # here will prompt you to enter access key and secret access key","title":"Setup the AWS CLI"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#clone-vulture-source-code","text":"Clone the source code of Vulture into your local computer git clone https://github.com/holab-hku/Vulture.git # change into directory below cd Vulture/nextflow # checkout the branch below for most updated code git checkout cloud-new-junyi","title":"Clone Vulture source code"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#create-s3-bucket-to-store-results","text":"# specify bucket names and save them into bash environment variables export BUCKET_NAME_TEMP=vulture-temp export BUCKET_NAME_RESULTS=vulture-results echo \"BUCKET_NAME_TEMP=${BUCKET_NAME_TEMP}\" |tee -a ~/.bashrc echo \"BUCKET_NAME_RESULTS=${BUCKET_NAME_RESULTS}\" |tee -a ~/.bashrc # create S3 buckets with specified bucket names aws --region ${AWS_REGION} s3 mb s3://${BUCKET_NAME_TEMP} aws --region ${AWS_REGION} s3 mb s3://${BUCKET_NAME_RESULTS}","title":"Create S3 Bucket to store results"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#run-vulture-pipeline-1-build-genome-reference","text":"Now we are about to run the first step of the Vulture pipeline i.e. mkref (genome reference making), execute the command below in your favourite terminal or powershell and wait it to be finished nextflow run scvh_mkref.nf -profile mkref -bucket-dir s3://${BUCKET_NAME_TEMP} --outdir=s3://${BUCKET_NAME_RESULTS}/batchA -with-report mkref_$(date +%s).html -bg &>> mkref_$(date +%s).log; The input data required for mkref stage are available in the downloadable links below, you can save them into your own S3 bucket folder: hg38.fa hg38.unique_gene_names.gtf prokaryotes.csv viruSITE_human_host.txt Also you can generate the files yourself following instructions below: VirusSITE (viruSITE human host) Click \"Format: CSV\" NCBI Prokaryotes Filters -> Host (Homo sapiens) -> Assembly level (Complete) -> RefSeq category (representative) -> Download [prokaryotes.csv] After the mkref job is done, you need to edit line in \"nextflow/nextflow.config\" file -> \"params.ref\" to the actual S3 path where your output reference genome files are i.e. in \"s3://${BUCKET_NAME_RESULTS}/batchA\" or you could download from the downloadable links below and store them into your own S3 bucket folder: human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.removed_amb_viral_exon.gtf human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.fa human_host_viruses.viruSITE.with_hg38.removed_amb_viral_exon.gtf human_host_viruses.viruSITE.with_hg38.fa ... mkref { aws.region = 'us-east-2' process.container = 'public.ecr.aws/b6a4h2a6/scvh_mkref:latest' process.executor = 'awsbatch' process.queue = 'vulture-stdq' # this line needs to be changed params.ref = 's3://vulture-reference/humangenome/' } ...","title":"Run Vulture pipeline - 1. Build Genome reference"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#run-vulture-pipeline-2-start-main-analysis","text":"Before we start our analysis, we need to edit \"nextflow/params.yaml\" file to include the reads of your interest. Here is a snippet of how the \"params.yaml\" file looks like ... soloStrand: \"Forward\" alignment: \"STAR\" technology: \"10XV2\" virus_database: \"viruSITE\" soloMultiMappers: \"EM\" soloFeatures: \"GeneFull\" inputformat: \"bam\" soloInputSAMattrBarcodeSeq: \"CB UB\" barcodes_whitelist: \"None\" reads: - \"SRR6885502\" - \"SRR6885503\" - \"SRR6885504\" - \"SRR6885505\" - \"SRR6885506\" - \"SRR6885507\" - \"SRR6885508\" ... Execute the command below to start the main analysis of Vulture. nextflow run scvh_full.nf -profile batchfull -params-file params.yaml -bucket-dir s3://${BUCKET_NAME_TEMP} --outdir=s3://${BUCKET_NAME_RESULTS}/batchD -with-report report_bam_$(date +%s).html -bg &>> submitnf_bam_$(date +%s).log [Previous Step](https://juychen.github.io/docs/3_Nextflow/NextflowInstall.html){: .btn } [Next Step](https://juychen.github.io/docs/6_Local/Localusage.html){: .btn .btn-purple }","title":"Run Vulture pipeline - 2. Start main analysis"},{"location":"3.%20Building%20your%20reference%20genome/1_Build/","text":"layout: default title: Build our own reference genome nav_order: 1","title":"1 Build"},{"location":"Supplementary/Launchtemp/","text":"Setup Launch Template Before we start, we need to setup a launch template of EC2 instance for our computing environments for AWS batch. Because we have mentioned in the previous section that the storage of EC2 instance is not enough for the scRNA-Seq data preprocessing. The launch template can be created in either GUI-way or through AWS Command Line (AWS CLI). To create the template in GUI-way, navigate to AWS EC2 dashboard at AWS EC2 Home . In the left panel menu, select \"Launch Templates\" under Instances. Alternatively, you can run the following command to create the launch template from CLI. The launch template file in our project (launch-template-data.json) is shown as follows: { \"LaunchTemplateName\": \"increase-volume\", \"LaunchTemplateData\": { \"BlockDeviceMappings\": [ { \"DeviceName\": \"/dev/xvda\", \"Ebs\": { \"VolumeSize\": 2500, \"VolumeType\": \"gp2\" } } ] } } With this template name \"vulture-launch-template\", we can increase the default storage of EC2 instances to 2500GB. We can run the following script: Run the command below to create this template. aws ec2 --region ${AWS_REGION} create-launch-template --cli-input-json launch-template-data.json","title":"Launch template Setup"},{"location":"Supplementary/Launchtemp/#setup-launch-template","text":"Before we start, we need to setup a launch template of EC2 instance for our computing environments for AWS batch. Because we have mentioned in the previous section that the storage of EC2 instance is not enough for the scRNA-Seq data preprocessing. The launch template can be created in either GUI-way or through AWS Command Line (AWS CLI). To create the template in GUI-way, navigate to AWS EC2 dashboard at AWS EC2 Home . In the left panel menu, select \"Launch Templates\" under Instances. Alternatively, you can run the following command to create the launch template from CLI. The launch template file in our project (launch-template-data.json) is shown as follows: { \"LaunchTemplateName\": \"increase-volume\", \"LaunchTemplateData\": { \"BlockDeviceMappings\": [ { \"DeviceName\": \"/dev/xvda\", \"Ebs\": { \"VolumeSize\": 2500, \"VolumeType\": \"gp2\" } } ] } } With this template name \"vulture-launch-template\", we can increase the default storage of EC2 instances to 2500GB. We can run the following script: Run the command below to create this template. aws ec2 --region ${AWS_REGION} create-launch-template --cli-input-json launch-template-data.json","title":"Setup Launch Template"},{"location":"Supplementary/Supplementary/","text":"A typical AWS Batch workload might be triggered by input data being uploaded to S3, this will kick in multiple stages. Overview The Upload event triggers a submission to a job queue (e.g. via lambda function) The AWS Batch scheduler runs periodically to check for jobs in the job queue Once jobs are placed, the scheduler evaluates the Compute Environments and adjusts the compute capacity to allow the jobs to run. The job output is stored external to the job (e.g. S3, EFS or FSx for Lustre)","title":"Supplementary"},{"location":"Supplementary/Supplementary/#overview","text":"The Upload event triggers a submission to a job queue (e.g. via lambda function) The AWS Batch scheduler runs periodically to check for jobs in the job queue Once jobs are placed, the scheduler evaluates the Compute Environments and adjusts the compute capacity to allow the jobs to run. The job output is stored external to the job (e.g. S3, EFS or FSx for Lustre)","title":"Overview"}]}