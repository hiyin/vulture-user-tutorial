{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Vulture tutorial The content of this tutorial is derived from a tutorial Nextflow with Batch . We changed the goal to run our Vulture pipeline, which we will run locally in containers and submit locally to AWS Batch. Overview In this tutorial, you will get hands-on experience with Vulture pipeline. The goal of this tutorial is to get familiar with using Vulture and its pipeline architecture based on the following AWS services. AWS EC2: cloud server instances AWS EC2 Spot Instances: EC2 instances benefit from unused EC2 capacity AWS Batch: Job scheduler of containerized workflow in batch AWS S3: Object storage Other concepts Docker: container services Nextflow: language reproducible scientific workflows","title":"Welcome"},{"location":"#welcome-to-the-vulture-tutorial","text":"The content of this tutorial is derived from a tutorial Nextflow with Batch . We changed the goal to run our Vulture pipeline, which we will run locally in containers and submit locally to AWS Batch.","title":"Welcome to the Vulture tutorial"},{"location":"#overview","text":"In this tutorial, you will get hands-on experience with Vulture pipeline. The goal of this tutorial is to get familiar with using Vulture and its pipeline architecture based on the following AWS services. AWS EC2: cloud server instances AWS EC2 Spot Instances: EC2 instances benefit from unused EC2 capacity AWS Batch: Job scheduler of containerized workflow in batch AWS S3: Object storage","title":"Overview"},{"location":"#other-concepts","text":"Docker: container services Nextflow: language reproducible scientific workflows","title":"Other concepts"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/","text":"Quick start PLACEHOLDER For Linux Machine Install R Pick your CRAN/mirror to start installation https://cran.r-project.org/mirrors.html Here we choose Hong Kong mirror at https://mirror-hk.koddos.net/CRAN/. Click [Download R for Linux], then choose \"ubuntu\" in the next page. You will be directed to the download page where you will see prompts below. Run these lines (if root, remove sudo) to tell Ubuntu about the R binaries at CRAN. # update indices sudo apt update -qq # install two helper packages we need sudo apt install --no-install-recommends software-properties-common dirmngr # add the signing key (by Michael Rutter) for these repos # To verify key, run gpg --show-keys /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc # Fingerprint: E298A3A825C0D65DFD57CBB651716619E084DAB9 wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc # add the R 4.0 repo from CRAN -- adjust 'focal' to 'groovy' or 'bionic' as needed sudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\" Install Docker Installation methods can be found exactly at https://docs.docker.com/engine/install/ubuntu/. You can install Docker Engine in different ways, depending on your needs: We install using the apt repository. Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository. # Set up the repository # Update the apt package index and install packages to allow apt to use a repository over HTTPS: sudo apt-get update sudo apt-get install ca-certificates curl gnupg # Add Docker\u2019s official GPG key: sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Use the following command to set up the repository: echo \\ \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install Docker Engine # Update the apt package index: sudo apt-get update # Install Docker Engine, containerd, and Docker Compose. # To install the latest version, run: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin #Verify that the Docker Engine installation is successful by running the hello-world image. sudo docker run hello-world # This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits. sudo apt install build-essential sudo apt-get update --fix-missing && \\ sudo apt-get install -y wget bzip2 ca-certificates \\ libglib2.0-0 libxext6 libsm6 libxrender1 curl grep sed dpkg libcurl4-openssl-dev libssl-dev libhdf5-dev \\ git mercurial subversion procps \\ libxml-libxml-perl pigz awscli uuid-runtime time tini sudo apt-get install libblas-dev liblapack-dev sudo apt-get install gfortran need sudo to install packages otherwise error sudo Rscript -e 'install.packages(\"BiocManager\")' sudo Rscript -e 'BiocManager::install(\"DropletUtils\")' Install Nextflow Install Java sudo apt install default-jdk wget -qO- https://get.nextflow.io | bash sudo apt-get install -y graphviz jq run from current directory chmod +x nextflow run from anywhere sudo mv nextflow /usr/local/bin/ install DropletUtils Install samtools need following dependencies ncurses sudo apt-get install libncurses5-dev sudo apt-get install libbz2-dev sudo apt-get install liblzma-dev dependenties other than R packages wget https://github.com/alexdobin/STAR/archive/2.7.9a.tar.gz && \\ tar -xzf 2.7.9a.tar.gz && \\ echo \"export PATH=/home/ubuntu/STAR-2.7.9a/bin/Linux_x86_64_static:\\$PATH\" >> ~/.bashrc && \\ wget https://github.com/samtools/samtools/releases/download/1.13/samtools-1.13.tar.bz2 && \\ tar -xf samtools-1.13.tar.bz2 && \\ cd samtools-1.13 && \\ ./configure --prefix=/home/ubuntu/samtools make && make install && echo \"export PATH=/home/ubuntu/samtools/bin:\\$PATH\" >> ~/.bashrc . ~/.bashrc cd $HOME sudo apt-get install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa sudo apt-get update sudo apt-get install python3.8 sudo apt-get install python3-pip pip install kb-python boto3 awscli download all files include whitelist in vmh_ mkdir vmh_genome_dir cd vmh_genome_dir wget https://vulture-reference.s3.ap-east-1.amazonaws.com/human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.removed_amb_viral_exon.gtf wget https://vulture-reference.s3.ap-east-1.amazonaws.com/human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.fa wget https://vulture-reference.s3.ap-east-1.amazonaws.com/human_host_viruses.viruSITE.with_hg38.removed_amb_viral_exon.gtf wget https://vulture-reference.s3.ap-east-1.amazonaws.com/human_host_viruses.viruSITE.with_hg38.fa wget https://vulture-reference.s3.ap-east-1.amazonaws.com/3M-february-2018.txt wget https://vulture-reference.s3.ap-east-1.amazonaws.com/737K-august-2016.txt cd $HOME wget https://vulture-reference.s3.ap-east-1.amazonaws.com/example/SRR12570425_1.fastq.gz wget https://vulture-reference.s3.ap-east-1.amazonaws.com/example/SRR12570425_2.fastq.gz cd $HOME git clone https://github.com/holab-hku/Vulture.git running command mkdir output directory otherwise error mkdir $HOME/Vulture_output mkdir $HOME/Vulture_output/SRR12570425 perl $HOME/Vulture/scvh_map_reads.pl -t 12 -o $HOME/Vulture_output/SRR12570425 $HOME/vmh_genome_dir $HOME/SRR12570425_2.fastq.gz $HOME/SRR12570425_1.fastq.gz --soloStrand \"Forward\" --whitelist \"$HOME/vmh_genome_dir/3M-february-2018.txt\" --soloCBlen 16 --soloUMIstart 1 --soloUMIstart 17 --soloUMIlen 12 -soloUMIlen 12 Rscript $HOME/Vulture/scvh_filter_matrix.r $HOME/Vulture_output/SRR12570425 perl $HOME/Vulture/scvh_analyze_bam.pl $HOME/Vulture_output/SRR12570425 Download nextflow nextflow download 22.10 package the tar.gz pack from github, untar , run ./nextflow in the package, then do export echo to path wget https://github.com/nextflow-io/nextflow/archive/refs/tags/v22.10.0.tar.gz tar xvf v22.10.0.tar.gz cd nextflow-22.10.0/ ./nextflow echo \"export PATH=/home/ubuntu/nextflow-22.10.0:\\$PATH\" >> ~/.bashrc nextflow run scvh_docker_local.nf -profile batchlocal -params-file params.yaml --outdir=/home/ubuntu/Vulture_output/nextflow -with-report nextflow_report_$(date +%s).html -bg &>> nextflow_log_$(d ate +%s).log","title":"Quick start"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#quick-start","text":"PLACEHOLDER For Linux Machine","title":"Quick start"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#install-r","text":"","title":"Install R"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#pick-your-cranmirror-to-start-installation","text":"https://cran.r-project.org/mirrors.html Here we choose Hong Kong mirror at https://mirror-hk.koddos.net/CRAN/. Click [Download R for Linux], then choose \"ubuntu\" in the next page. You will be directed to the download page where you will see prompts below. Run these lines (if root, remove sudo) to tell Ubuntu about the R binaries at CRAN. # update indices sudo apt update -qq # install two helper packages we need sudo apt install --no-install-recommends software-properties-common dirmngr # add the signing key (by Michael Rutter) for these repos # To verify key, run gpg --show-keys /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc # Fingerprint: E298A3A825C0D65DFD57CBB651716619E084DAB9 wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc # add the R 4.0 repo from CRAN -- adjust 'focal' to 'groovy' or 'bionic' as needed sudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\"","title":"Pick your CRAN/mirror to start installation"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#install-docker","text":"Installation methods can be found exactly at https://docs.docker.com/engine/install/ubuntu/. You can install Docker Engine in different ways, depending on your needs: We install using the apt repository. Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository. # Set up the repository # Update the apt package index and install packages to allow apt to use a repository over HTTPS: sudo apt-get update sudo apt-get install ca-certificates curl gnupg # Add Docker\u2019s official GPG key: sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Use the following command to set up the repository: echo \\ \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null","title":"Install Docker"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#install-docker-engine","text":"# Update the apt package index: sudo apt-get update # Install Docker Engine, containerd, and Docker Compose. # To install the latest version, run: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin #Verify that the Docker Engine installation is successful by running the hello-world image. sudo docker run hello-world # This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits. sudo apt install build-essential sudo apt-get update --fix-missing && \\ sudo apt-get install -y wget bzip2 ca-certificates \\ libglib2.0-0 libxext6 libsm6 libxrender1 curl grep sed dpkg libcurl4-openssl-dev libssl-dev libhdf5-dev \\ git mercurial subversion procps \\ libxml-libxml-perl pigz awscli uuid-runtime time tini sudo apt-get install libblas-dev liblapack-dev sudo apt-get install gfortran","title":"Install Docker Engine"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#need-sudo-to-install-packages-otherwise-error","text":"sudo Rscript -e 'install.packages(\"BiocManager\")' sudo Rscript -e 'BiocManager::install(\"DropletUtils\")'","title":"need sudo to install packages otherwise error"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#install-nextflow","text":"","title":"Install Nextflow"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#install-java","text":"sudo apt install default-jdk wget -qO- https://get.nextflow.io | bash sudo apt-get install -y graphviz jq","title":"Install Java"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#run-from-current-directory","text":"chmod +x nextflow","title":"run from current directory"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#run-from-anywhere","text":"sudo mv nextflow /usr/local/bin/","title":"run from anywhere"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#install-dropletutils","text":"","title":"install DropletUtils"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#install-samtools-need-following-dependencies-ncurses","text":"sudo apt-get install libncurses5-dev sudo apt-get install libbz2-dev sudo apt-get install liblzma-dev","title":"Install samtools need following dependencies ncurses"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#dependenties-other-than-r-packages","text":"wget https://github.com/alexdobin/STAR/archive/2.7.9a.tar.gz && \\ tar -xzf 2.7.9a.tar.gz && \\ echo \"export PATH=/home/ubuntu/STAR-2.7.9a/bin/Linux_x86_64_static:\\$PATH\" >> ~/.bashrc && \\ wget https://github.com/samtools/samtools/releases/download/1.13/samtools-1.13.tar.bz2 && \\ tar -xf samtools-1.13.tar.bz2 && \\ cd samtools-1.13 && \\ ./configure --prefix=/home/ubuntu/samtools make && make install && echo \"export PATH=/home/ubuntu/samtools/bin:\\$PATH\" >> ~/.bashrc . ~/.bashrc cd $HOME sudo apt-get install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa sudo apt-get update sudo apt-get install python3.8 sudo apt-get install python3-pip pip install kb-python boto3 awscli","title":"dependenties other than R packages"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#download-all-files-include-whitelist-in-vmh_","text":"mkdir vmh_genome_dir cd vmh_genome_dir wget https://vulture-reference.s3.ap-east-1.amazonaws.com/human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.removed_amb_viral_exon.gtf wget https://vulture-reference.s3.ap-east-1.amazonaws.com/human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.fa wget https://vulture-reference.s3.ap-east-1.amazonaws.com/human_host_viruses.viruSITE.with_hg38.removed_amb_viral_exon.gtf wget https://vulture-reference.s3.ap-east-1.amazonaws.com/human_host_viruses.viruSITE.with_hg38.fa wget https://vulture-reference.s3.ap-east-1.amazonaws.com/3M-february-2018.txt wget https://vulture-reference.s3.ap-east-1.amazonaws.com/737K-august-2016.txt cd $HOME wget https://vulture-reference.s3.ap-east-1.amazonaws.com/example/SRR12570425_1.fastq.gz wget https://vulture-reference.s3.ap-east-1.amazonaws.com/example/SRR12570425_2.fastq.gz cd $HOME git clone https://github.com/holab-hku/Vulture.git","title":"download all files include whitelist in vmh_"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#running-command","text":"","title":"running command"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#mkdir-output-directory-otherwise-error","text":"mkdir $HOME/Vulture_output mkdir $HOME/Vulture_output/SRR12570425 perl $HOME/Vulture/scvh_map_reads.pl -t 12 -o $HOME/Vulture_output/SRR12570425 $HOME/vmh_genome_dir $HOME/SRR12570425_2.fastq.gz $HOME/SRR12570425_1.fastq.gz --soloStrand \"Forward\" --whitelist \"$HOME/vmh_genome_dir/3M-february-2018.txt\" --soloCBlen 16 --soloUMIstart 1 --soloUMIstart 17 --soloUMIlen 12 -soloUMIlen 12","title":"mkdir output directory otherwise error"},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#_1","text":"Rscript $HOME/Vulture/scvh_filter_matrix.r $HOME/Vulture_output/SRR12570425","title":""},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#_2","text":"perl $HOME/Vulture/scvh_analyze_bam.pl $HOME/Vulture_output/SRR12570425","title":""},{"location":"1.%20Running%20on%20local%20computer/1_Quickstart/#download-nextflow","text":"nextflow download 22.10 package the tar.gz pack from github, untar , run ./nextflow in the package, then do export echo to path wget https://github.com/nextflow-io/nextflow/archive/refs/tags/v22.10.0.tar.gz tar xvf v22.10.0.tar.gz cd nextflow-22.10.0/ ./nextflow echo \"export PATH=/home/ubuntu/nextflow-22.10.0:\\$PATH\" >> ~/.bashrc nextflow run scvh_docker_local.nf -profile batchlocal -params-file params.yaml --outdir=/home/ubuntu/Vulture_output/nextflow -with-report nextflow_report_$(date +%s).html -bg &>> nextflow_log_$(d ate +%s).log","title":"Download nextflow"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/","text":"Create Batch Compute Environmnet Requirements Input data: 10x Chromium scRNA-seq reads DropletUtils >= v1.10.2 STAR >= v2.7.9a or cellranger >= 6.0.0 or Kallisto/bustools >= 0.25.1 or salmon/alevin >= v1.4.0 General usage Map 10x scRNA-seq reads to the viral (and microbial) host reference set using STARsolo, CellRanger, Kallisto|bustools, or Salmon|Alevin. 0. Prerequiresits to download genome files You need to download virus genome, prokaryotes genome, combined genome and virus combined genome in the following link and save them in a folder as \"vmh_genome_dir\" to be used in the next step. human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.removed_amb_viral_exon.gtf human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.fa human_host_viruses.viruSITE.with_hg38.removed_amb_viral_exon.gtf human_host_viruses.viruSITE.with_hg38.fa 1. Map 10x scRNA-seq reads to the viral microbial host reference set: Usage: scvh_map_reads.pl [Options] <vmh_genome_dir> <R2> <R1> or <vmh_genome_dir> <.bam file> Options: Defaults -o/--output-dir <string> the output directory [./] -t/--threads <int> number of threads to run alignment with [<1>] -d/--database <string> select virus or virus and prokaryotes database, can be 'viruSITE' or 'viruSITE.NCBIprokaryotes' [<viruSITE.NCBIprokaryotes>] -e/--exe <string> executable command or stand alone executable path of the alignment tool [<>] -s/--soloStrand <string> STARsolo param: Reverse or Forward used for 10x 5' or 3' protocol, respectively [<Reverse>] -w/--whitelist <string> STARsolo param --soloCBwhitelist [<\"vmh_genome_dir\"/737K-august-2016.txt>] -r/--ram <int> limitation of RAM usage. For STARsolo, param: limitGenomeGenerateRAM, limitBAMsortRAM unit by GB [<128>] -f/--soloFeature <string> STARsolo param: See --soloFeatures in STARsolo manual [<Gene>] -ot/--outSAMtype <string> STARsolo param: See --outSAMtype in STARsolo manual [<BAM SortedByCoordinate>] -mm/--soloMultiMappers <string> STARsolo param: See --soloMultiMappers in STARsolo manual [<EM>] -a/--alignment <string> Select alignment methods: 'STAR', 'KB', 'Alevin', or 'CellRanger' [<STAR>] -v/--technology <string> KB param: Single-cell technology used (`kb --list` to view) [<10XV2>] --soloCBstart <string> STARsolo param: See --soloCBstart in STARsolo manual [<1>] --soloCBlen <string> STARsolo param: See --soloCBlen in STARsolo manual [<16>] --soloUMIstart <string> STARsolo param: See --soloUMIstart in STARsolo manual [<17>] --soloUMIlen <string> STARsolo param: See --soloUMIlen in STARsolo manual [<10>] --soloInputSAMattrBarcodeSeq <string> STARsolo param: See --soloInputSAMattrBarcodeSeq in STARsolo manual [<CR UR>] For fastqs alignment option 'STAR', 'KB', and 'Alevin', run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir R2.fastq.gz R1.fastq.gz where -t is a user-specified integer indicating number of threads to run with, output_dir is a user-specified directory to place the outputs, vmh_genome_dir is a pre-generated viral (and microbial) host (human) reference set directory, R2.fastq.gz and R1.fastq.gz are input 10x scRNA-seq reads. For fastqs option 'CellRanger', run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir sample fastqs where sample and fastqs are two cellranger arguments: --sample and --fastqs . See documentation in cellranger count to infer rules of fastq and sample naming. For bam files, we only support STARsolo, run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir your_bam_file.bam 2. Filter the mapped UMIs using EmptyDrops to get the viral (and microbial) host filtered UMI counts matrix and also output viral genes and barcodes info files: Usage: Rscript scvh_filter_matrix.r output_dir sample_name where sample_name is an optional user-specified tag to be used as a prefix for the output files. 3. (Optional, and STARsolo or CellRanger only) Output some quality control criteria of the post-EmptyDrops viral microbial supporting reads in the BAM file Usage: perl scvh_analyze_bam.pl output_dir sample_name","title":"Command line"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#create-batch-compute-environmnet","text":"","title":"Create Batch Compute Environmnet"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#requirements","text":"Input data: 10x Chromium scRNA-seq reads DropletUtils >= v1.10.2 STAR >= v2.7.9a or cellranger >= 6.0.0 or Kallisto/bustools >= 0.25.1 or salmon/alevin >= v1.4.0","title":"Requirements"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#general-usage","text":"Map 10x scRNA-seq reads to the viral (and microbial) host reference set using STARsolo, CellRanger, Kallisto|bustools, or Salmon|Alevin.","title":"General usage"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#0-prerequiresits-to-download-genome-files","text":"You need to download virus genome, prokaryotes genome, combined genome and virus combined genome in the following link and save them in a folder as \"vmh_genome_dir\" to be used in the next step. human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.removed_amb_viral_exon.gtf human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.fa human_host_viruses.viruSITE.with_hg38.removed_amb_viral_exon.gtf human_host_viruses.viruSITE.with_hg38.fa","title":"0. Prerequiresits to download genome files"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#1-map-10x-scrna-seq-reads-to-the-viral-microbial-host-reference-set","text":"Usage: scvh_map_reads.pl [Options] <vmh_genome_dir> <R2> <R1> or <vmh_genome_dir> <.bam file> Options: Defaults -o/--output-dir <string> the output directory [./] -t/--threads <int> number of threads to run alignment with [<1>] -d/--database <string> select virus or virus and prokaryotes database, can be 'viruSITE' or 'viruSITE.NCBIprokaryotes' [<viruSITE.NCBIprokaryotes>] -e/--exe <string> executable command or stand alone executable path of the alignment tool [<>] -s/--soloStrand <string> STARsolo param: Reverse or Forward used for 10x 5' or 3' protocol, respectively [<Reverse>] -w/--whitelist <string> STARsolo param --soloCBwhitelist [<\"vmh_genome_dir\"/737K-august-2016.txt>] -r/--ram <int> limitation of RAM usage. For STARsolo, param: limitGenomeGenerateRAM, limitBAMsortRAM unit by GB [<128>] -f/--soloFeature <string> STARsolo param: See --soloFeatures in STARsolo manual [<Gene>] -ot/--outSAMtype <string> STARsolo param: See --outSAMtype in STARsolo manual [<BAM SortedByCoordinate>] -mm/--soloMultiMappers <string> STARsolo param: See --soloMultiMappers in STARsolo manual [<EM>] -a/--alignment <string> Select alignment methods: 'STAR', 'KB', 'Alevin', or 'CellRanger' [<STAR>] -v/--technology <string> KB param: Single-cell technology used (`kb --list` to view) [<10XV2>] --soloCBstart <string> STARsolo param: See --soloCBstart in STARsolo manual [<1>] --soloCBlen <string> STARsolo param: See --soloCBlen in STARsolo manual [<16>] --soloUMIstart <string> STARsolo param: See --soloUMIstart in STARsolo manual [<17>] --soloUMIlen <string> STARsolo param: See --soloUMIlen in STARsolo manual [<10>] --soloInputSAMattrBarcodeSeq <string> STARsolo param: See --soloInputSAMattrBarcodeSeq in STARsolo manual [<CR UR>] For fastqs alignment option 'STAR', 'KB', and 'Alevin', run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir R2.fastq.gz R1.fastq.gz where -t is a user-specified integer indicating number of threads to run with, output_dir is a user-specified directory to place the outputs, vmh_genome_dir is a pre-generated viral (and microbial) host (human) reference set directory, R2.fastq.gz and R1.fastq.gz are input 10x scRNA-seq reads. For fastqs option 'CellRanger', run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir sample fastqs where sample and fastqs are two cellranger arguments: --sample and --fastqs . See documentation in cellranger count to infer rules of fastq and sample naming. For bam files, we only support STARsolo, run: perl scvh_map_reads.pl -t num_threads -o output_dir vmh_genome_dir your_bam_file.bam","title":"1. Map 10x scRNA-seq reads to the viral microbial host reference set:"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#2-filter-the-mapped-umis-using-emptydrops-to-get-the-viral-and-microbial-host-filtered-umi-counts-matrix-and-also-output-viral-genes-and-barcodes-info-files","text":"Usage: Rscript scvh_filter_matrix.r output_dir sample_name where sample_name is an optional user-specified tag to be used as a prefix for the output files.","title":"2. Filter the mapped UMIs using EmptyDrops to get the viral (and microbial) host filtered UMI counts matrix and also output viral genes and barcodes info files:"},{"location":"1.%20Running%20on%20local%20computer/2_Commandline/#3-optional-and-starsolo-or-cellranger-only-output-some-quality-control-criteria-of-the-post-emptydrops-viral-microbial-supporting-reads-in-the-bam-file","text":"Usage: perl scvh_analyze_bam.pl output_dir sample_name","title":"3. (Optional, and STARsolo or CellRanger only) Output some quality control criteria of the post-EmptyDrops viral microbial supporting reads in the BAM file"},{"location":"1.%20Running%20on%20local%20computer/3_Nextflow/","text":"Install the dependencies for the pipeline PLACE HOLDER 2 Requirements Input data: 10x Chromium scRNA-seq reads DropletUtils >= v1.10.2 STAR >= v2.7.9a or cellranger >= 6.0.0 or Kallisto/bustools >= 0.25.1 or salmon/alevin >= v1.4.0 Nextflow >= v21.04.3 The instructions are tested on the following system: Distributor ID: Ubuntu Description: Ubuntu 22.04.2 LTS Release: 22.04 Codename: jammy Specify the local envrionment in the Nextflow config file Open the \"nextflow.config\" file in the vulture/nextflow directory. This following snippet shows how to disable the docker container for the pipeline. ... batchlocal { docker.enabled = false } ... Specify the configuration file for an analysis reading files from the local computer Before we start our analysis, we need to creat a configuration file for the analysis. Here is a snippet of how the \"params.yaml\" file looks like: ... soloStrand: \"Forward\" alignment: \"STAR\" technology: \"10XV3\" virus_database: \"viruSITE.NCBIprokaryotes\" soloMultiMappers: \"EM\" soloFeatures: \"GeneFull\" inputformat: \"fastq\" sampleSubfix1: \"_1\" sampleSubfix2: \"_2\" ref: [The full path of your reference genome direcory, e.g. /home/user/data/references] samplepath: [The full path of your fastq samples, e.g /home/user/data/fastq] read2urls: - [The full path of your _2.fastq.gz file, e.g. /home/user/data/fastq/SRR12570125_2.fastq.gz] read1urls: - [The full path of your _1.fastq.gz file, e.g. /home/user/data/fastq/SRR12570125_1.fastq.gz] reads: - [An unique ID of your sample, e.g SRR12570125] ... Execute the command below to start the main analysis of Vulture. cd vulture/nextflow nextflow run scvh_docker_local.nf -profile batchlocal -params-file params.yaml --outdir=your_output_directory -with-report nextflow_report_$(date +%s).html -bg &>> nextflow_log_$(date +%s).log A successful run will generate the following files in the output directory: ... nextflow_report_1628188800.html nextflow_log_1628188800.log ... The \"nextflow_report_1628188800.html\" file is a report of the analysis. The \"nextflow_log_1628188800.log\" file is the log file of the analysis. A successful run will also generate the following files in the nextflow_log_1628188800.log: N E X T F L O W ~ version 21.10.6 Launching `scvh_docker_local.nf` [cheeky_brown] - revision: 59a6446081 S C V H - N F P I P E L I N E =================================== transcriptome: /mnt/d/scvh_files/vmh_genome_dir/references reads : [SRR12570125] outdir : /mnt/d/output/ database: : viruSITE.NCBIprokaryotes threads : 10 ram : 128 alignment : STAR whitelist : 3M-february-2018.txt soloCBlen : 16 soloCBstart : 1 soloUMIstart : 17 soloUMIlen : 12 soloStrand : Forward soloMultiMappers: EM soloFeature : GeneFull outSAMtype : BAM SortedByCoordinate technology : 10XV3 pseudoBAM : inputformat : fastq sampleSubfix1 : _1 sampleSubfix2 : _2 [SRR12570125, /mnt/d/scvh_files/EXAMPLES/SRR12570125_1.fastq.gz, /mnt/d/scvh_files/EXAMPLES/SRR12570125_2.fastq.gz] [88/9dd405] Submitted process > Map (1) ... Completed at: 14-Jul-2023 16:56:53 Duration : 16m 8s CPU hours : 57.6 (1.4% failed) Succeeded : 3 Specify the configuration file for an analysis reading files from the SRA database Alternatively, you can also download the fastq files from the SRA database. Here is a snippet of how the \"params.yaml\" file looks like: alignment: STAR codebase: [The full path of your Vulture direcory, e.g. /home/user/code/Vulture] inputformat: fastq ram: 128 ref: [The full path of your reference genome direcory, e.g. /home/user/data/references] reads: - SRR14736914 - SRR14736920 - SRR14736921 - SRR14736923 - SRR14736925 - SRR14736934 - SRR14736927 - SRR14736936 soloFeatures: GeneFull soloStrand: Reverse technology: 10XV3 virus_database: viruSITE.NCBIprokaryotes Execute the command below to start the main analysis of Vulture. cd vulture/nextflow nextflow run scvh_full_local.nf -profile batchlocal -params-file params.yaml --outdir=your_output_directory -with-report nextflow_report_$(date +%s).html -bg &>> nextflow_log_$(date +%s).log The analysis will launch the SRA-tools and dump fastq files from the SRA database and start the analysis. A successful run will generate the following files in the output directory: N E X T F L O W ~ version 21.10.6 Launching `scvh_full_local.nf` [backstabbing_austin] - revision: a459ae3e2c S C V H - N F P I P E L I N E =================================== transcriptome: [The full path of your Vulture direcory, e.g. /home/user/code/Vulture] reads : [SRR14736914, SRR14736920, SRR14736921, SRR14736923, SRR14736925, SRR14736934, SRR14736927, SRR14736936] outdir : [The full path of your Vulture direcory, e.g. /home/user/output/Vulture] database: : viruSITE.NCBIprokaryotes threads : 10 ram : 128 alignment : STAR whitelist : 3M-february-2018.txt soloCBlen : 16 soloCBstart : 1 soloUMIstart : 17 soloUMIlen : 12 soloStrand : Reverse soloMultiMappers: EM soloFeature : GeneFull outSAMtype : BAM SortedByCoordinate technology : 10XV3 pseudoBAM : inputformat : fastq sampleSubfix1 : _1 sampleSubfix2 : _2 SRR14736914 SRR14736920 SRR14736921 SRR14736923 SRR14736925 SRR14736934 SRR14736927 SRR14736936 executor > local (1) [2f/894239] process > Dump (6) [ 0%] 0 of 8 [- ] process > Map - [- ] process > Filter - [- ] process > Analysis - executor > local (1) [2f/894239] process > Dump (6) [ 0%] 0 of 8 [- ] process > Map - [- ] process > Filter - [- ] process > Analysis - executor > local (2) [f6/245efd] process > Dump (7) [ 0%] 0 of 8 [- ] process > Map - [- ] process > Filter - [- ] process > Analysis - executor > local (2) [2f/894239] process > Dump (6) [ 12%] 1 of 8 [- ] process > Map - [- ] process > Filter - [- ] process > Analysis - executor > local (2) [2f/894239] process > Dump (6) [ 12%] 1 of 8 [- ] process > Map - [- ] process > Filter - [- ] process > Analysis - executor > local (2) [2f/894239] process > Dump (6) [ 12%] 1 of 8 [- ] process > Map [ 0%] 0 of 1 [- ] process > Filter - [- ] process > Analysis - .... Completed at: 14-Jul-2023 16:56:53 Duration : 3h 36m 8s CPU hours : 57.6 (1.4% failed) Succeeded : 8 Ignored : 8 Failed : 8 [Previous Step](https://juychen.github.io/docs/3_Nextflow/NextflowInstall.html){: .btn } [Next Step](https://juychen.github.io/docs/6_Local/Localusage.html){: .btn .btn-purple }","title":"Nextflow"},{"location":"1.%20Running%20on%20local%20computer/3_Nextflow/#install-the-dependencies-for-the-pipeline","text":"PLACE HOLDER 2","title":"Install the dependencies for the pipeline"},{"location":"1.%20Running%20on%20local%20computer/3_Nextflow/#requirements","text":"Input data: 10x Chromium scRNA-seq reads DropletUtils >= v1.10.2 STAR >= v2.7.9a or cellranger >= 6.0.0 or Kallisto/bustools >= 0.25.1 or salmon/alevin >= v1.4.0 Nextflow >= v21.04.3 The instructions are tested on the following system: Distributor ID: Ubuntu Description: Ubuntu 22.04.2 LTS Release: 22.04 Codename: jammy","title":"Requirements"},{"location":"1.%20Running%20on%20local%20computer/3_Nextflow/#specify-the-local-envrionment-in-the-nextflow-config-file","text":"Open the \"nextflow.config\" file in the vulture/nextflow directory. This following snippet shows how to disable the docker container for the pipeline. ... batchlocal { docker.enabled = false } ...","title":"Specify the local envrionment in the Nextflow config file"},{"location":"1.%20Running%20on%20local%20computer/3_Nextflow/#specify-the-configuration-file-for-an-analysis-reading-files-from-the-local-computer","text":"Before we start our analysis, we need to creat a configuration file for the analysis. Here is a snippet of how the \"params.yaml\" file looks like: ... soloStrand: \"Forward\" alignment: \"STAR\" technology: \"10XV3\" virus_database: \"viruSITE.NCBIprokaryotes\" soloMultiMappers: \"EM\" soloFeatures: \"GeneFull\" inputformat: \"fastq\" sampleSubfix1: \"_1\" sampleSubfix2: \"_2\" ref: [The full path of your reference genome direcory, e.g. /home/user/data/references] samplepath: [The full path of your fastq samples, e.g /home/user/data/fastq] read2urls: - [The full path of your _2.fastq.gz file, e.g. /home/user/data/fastq/SRR12570125_2.fastq.gz] read1urls: - [The full path of your _1.fastq.gz file, e.g. /home/user/data/fastq/SRR12570125_1.fastq.gz] reads: - [An unique ID of your sample, e.g SRR12570125] ... Execute the command below to start the main analysis of Vulture. cd vulture/nextflow nextflow run scvh_docker_local.nf -profile batchlocal -params-file params.yaml --outdir=your_output_directory -with-report nextflow_report_$(date +%s).html -bg &>> nextflow_log_$(date +%s).log A successful run will generate the following files in the output directory: ... nextflow_report_1628188800.html nextflow_log_1628188800.log ... The \"nextflow_report_1628188800.html\" file is a report of the analysis. The \"nextflow_log_1628188800.log\" file is the log file of the analysis. A successful run will also generate the following files in the nextflow_log_1628188800.log: N E X T F L O W ~ version 21.10.6 Launching `scvh_docker_local.nf` [cheeky_brown] - revision: 59a6446081 S C V H - N F P I P E L I N E =================================== transcriptome: /mnt/d/scvh_files/vmh_genome_dir/references reads : [SRR12570125] outdir : /mnt/d/output/ database: : viruSITE.NCBIprokaryotes threads : 10 ram : 128 alignment : STAR whitelist : 3M-february-2018.txt soloCBlen : 16 soloCBstart : 1 soloUMIstart : 17 soloUMIlen : 12 soloStrand : Forward soloMultiMappers: EM soloFeature : GeneFull outSAMtype : BAM SortedByCoordinate technology : 10XV3 pseudoBAM : inputformat : fastq sampleSubfix1 : _1 sampleSubfix2 : _2 [SRR12570125, /mnt/d/scvh_files/EXAMPLES/SRR12570125_1.fastq.gz, /mnt/d/scvh_files/EXAMPLES/SRR12570125_2.fastq.gz] [88/9dd405] Submitted process > Map (1) ... Completed at: 14-Jul-2023 16:56:53 Duration : 16m 8s CPU hours : 57.6 (1.4% failed) Succeeded : 3","title":"Specify the configuration file for an analysis reading files from the local computer"},{"location":"1.%20Running%20on%20local%20computer/3_Nextflow/#specify-the-configuration-file-for-an-analysis-reading-files-from-the-sra-database","text":"Alternatively, you can also download the fastq files from the SRA database. Here is a snippet of how the \"params.yaml\" file looks like: alignment: STAR codebase: [The full path of your Vulture direcory, e.g. /home/user/code/Vulture] inputformat: fastq ram: 128 ref: [The full path of your reference genome direcory, e.g. /home/user/data/references] reads: - SRR14736914 - SRR14736920 - SRR14736921 - SRR14736923 - SRR14736925 - SRR14736934 - SRR14736927 - SRR14736936 soloFeatures: GeneFull soloStrand: Reverse technology: 10XV3 virus_database: viruSITE.NCBIprokaryotes Execute the command below to start the main analysis of Vulture. cd vulture/nextflow nextflow run scvh_full_local.nf -profile batchlocal -params-file params.yaml --outdir=your_output_directory -with-report nextflow_report_$(date +%s).html -bg &>> nextflow_log_$(date +%s).log The analysis will launch the SRA-tools and dump fastq files from the SRA database and start the analysis. A successful run will generate the following files in the output directory: N E X T F L O W ~ version 21.10.6 Launching `scvh_full_local.nf` [backstabbing_austin] - revision: a459ae3e2c S C V H - N F P I P E L I N E =================================== transcriptome: [The full path of your Vulture direcory, e.g. /home/user/code/Vulture] reads : [SRR14736914, SRR14736920, SRR14736921, SRR14736923, SRR14736925, SRR14736934, SRR14736927, SRR14736936] outdir : [The full path of your Vulture direcory, e.g. /home/user/output/Vulture] database: : viruSITE.NCBIprokaryotes threads : 10 ram : 128 alignment : STAR whitelist : 3M-february-2018.txt soloCBlen : 16 soloCBstart : 1 soloUMIstart : 17 soloUMIlen : 12 soloStrand : Reverse soloMultiMappers: EM soloFeature : GeneFull outSAMtype : BAM SortedByCoordinate technology : 10XV3 pseudoBAM : inputformat : fastq sampleSubfix1 : _1 sampleSubfix2 : _2 SRR14736914 SRR14736920 SRR14736921 SRR14736923 SRR14736925 SRR14736934 SRR14736927 SRR14736936 executor > local (1) [2f/894239] process > Dump (6) [ 0%] 0 of 8 [- ] process > Map - [- ] process > Filter - [- ] process > Analysis - executor > local (1) [2f/894239] process > Dump (6) [ 0%] 0 of 8 [- ] process > Map - [- ] process > Filter - [- ] process > Analysis - executor > local (2) [f6/245efd] process > Dump (7) [ 0%] 0 of 8 [- ] process > Map - [- ] process > Filter - [- ] process > Analysis - executor > local (2) [2f/894239] process > Dump (6) [ 12%] 1 of 8 [- ] process > Map - [- ] process > Filter - [- ] process > Analysis - executor > local (2) [2f/894239] process > Dump (6) [ 12%] 1 of 8 [- ] process > Map - [- ] process > Filter - [- ] process > Analysis - executor > local (2) [2f/894239] process > Dump (6) [ 12%] 1 of 8 [- ] process > Map [ 0%] 0 of 1 [- ] process > Filter - [- ] process > Analysis - .... Completed at: 14-Jul-2023 16:56:53 Duration : 3h 36m 8s CPU hours : 57.6 (1.4% failed) Succeeded : 8 Ignored : 8 Failed : 8 [Previous Step](https://juychen.github.io/docs/3_Nextflow/NextflowInstall.html){: .btn } [Next Step](https://juychen.github.io/docs/6_Local/Localusage.html){: .btn .btn-purple }","title":"Specify the configuration file for an analysis reading files from the SRA database"},{"location":"1.%20Running%20on%20local%20computer/4_Docker/","text":"Run Vulture on local machines using docker The following instructions are for running Vulture on local machines using docker. The instructions are tested on the following system: Distributor ID: Ubuntu Description: Ubuntu 22.04.2 LTS Release: 22.04 Codename: jammy Install docker Place holder for docker installation Specify docker container in Nextflow config file Open the \"nextflow.config\" file in the vulture/nextflow directory. This following snippet shows how to specify the docker container for the pipeline. The docker container is hosted on Docker Hub and can be pulled by Nextflow automatically. ... dockerlocal { docker.enabled = true process.container = 'junyichen6/vulture:0.0.1' docker.fixOwnership = true docker.containerOptions = \"--user root\" } ... Specify the configuration file for an analysis Before we start our analysis, we need to creat a configuration file for the analysis. Here is a snippet of how the \"params.yaml\" file looks like: ... soloStrand: \"Forward\" alignment: \"STAR\" technology: \"10XV3\" virus_database: \"viruSITE.NCBIprokaryotes\" soloMultiMappers: \"EM\" soloFeatures: \"GeneFull\" inputformat: \"fastq\" sampleSubfix1: \"_1\" sampleSubfix2: \"_2\" ref: [The full path of your reference genome direcory, e.g. /home/user/data/references] samplepath: [The full path of your fastq samples, e.g /home/user/data/fastq] read2urls: - [The full path of your _2.fastq.gz file, e.g. /home/user/data/fastq/SRR12570125_2.fastq.gz] read1urls: - [The full path of your _1.fastq.gz file, e.g. /home/user/data/fastq/SRR12570125_1.fastq.gz] reads: - [An unique ID of your sample, e.g SRR12570125] ... Execute the command below to start the main analysis of Vulture. cd vulture/nextflow nextflow run scvh_docker_local.nf -profile dockerlocal -params-file params.yaml --outdir=your_output_directory -with-report nextflow_report_$(date +%s).html -bg &>> nextflow_log_$(date +%s).log A successful run will generate the following files in the output directory: ... nextflow_report_1628188800.html nextflow_log_1628188800.log ... The \"nextflow_report_1628188800.html\" file is a report of the analysis. The \"nextflow_log_1628188800.log\" file is the log file of the analysis. A successful run will also generate the following files in the nextflow_log_1628188800.log: N E X T F L O W ~ version 21.10.6 Launching `scvh_docker_local.nf` [cheeky_brown] - revision: 59a6446081 S C V H - N F P I P E L I N E =================================== transcriptome: /mnt/d/scvh_files/vmh_genome_dir/references reads : [SRR12570125] outdir : /mnt/d/output/ database: : viruSITE.NCBIprokaryotes threads : 10 ram : 128 alignment : STAR whitelist : 3M-february-2018.txt soloCBlen : 16 soloCBstart : 1 soloUMIstart : 17 soloUMIlen : 12 soloStrand : Forward soloMultiMappers: EM soloFeature : GeneFull outSAMtype : BAM SortedByCoordinate technology : 10XV3 pseudoBAM : inputformat : fastq sampleSubfix1 : _1 sampleSubfix2 : _2 [SRR12570125, /mnt/d/scvh_files/EXAMPLES/SRR12570125_1.fastq.gz, /mnt/d/scvh_files/EXAMPLES/SRR12570125_2.fastq.gz] [88/9dd405] Submitted process > Map (1) [Previous Step](https://juychen.github.io/docs/3_Nextflow/NextflowInstall.html){: .btn } [Next Step](https://juychen.github.io/docs/6_Local/Localusage.html){: .btn .btn-purple }","title":"Docker"},{"location":"1.%20Running%20on%20local%20computer/4_Docker/#run-vulture-on-local-machines-using-docker","text":"The following instructions are for running Vulture on local machines using docker. The instructions are tested on the following system: Distributor ID: Ubuntu Description: Ubuntu 22.04.2 LTS Release: 22.04 Codename: jammy","title":"Run Vulture on local machines using docker"},{"location":"1.%20Running%20on%20local%20computer/4_Docker/#install-docker","text":"Place holder for docker installation","title":"Install docker"},{"location":"1.%20Running%20on%20local%20computer/4_Docker/#specify-docker-container-in-nextflow-config-file","text":"Open the \"nextflow.config\" file in the vulture/nextflow directory. This following snippet shows how to specify the docker container for the pipeline. The docker container is hosted on Docker Hub and can be pulled by Nextflow automatically. ... dockerlocal { docker.enabled = true process.container = 'junyichen6/vulture:0.0.1' docker.fixOwnership = true docker.containerOptions = \"--user root\" } ...","title":"Specify docker container in Nextflow config file"},{"location":"1.%20Running%20on%20local%20computer/4_Docker/#specify-the-configuration-file-for-an-analysis","text":"Before we start our analysis, we need to creat a configuration file for the analysis. Here is a snippet of how the \"params.yaml\" file looks like: ... soloStrand: \"Forward\" alignment: \"STAR\" technology: \"10XV3\" virus_database: \"viruSITE.NCBIprokaryotes\" soloMultiMappers: \"EM\" soloFeatures: \"GeneFull\" inputformat: \"fastq\" sampleSubfix1: \"_1\" sampleSubfix2: \"_2\" ref: [The full path of your reference genome direcory, e.g. /home/user/data/references] samplepath: [The full path of your fastq samples, e.g /home/user/data/fastq] read2urls: - [The full path of your _2.fastq.gz file, e.g. /home/user/data/fastq/SRR12570125_2.fastq.gz] read1urls: - [The full path of your _1.fastq.gz file, e.g. /home/user/data/fastq/SRR12570125_1.fastq.gz] reads: - [An unique ID of your sample, e.g SRR12570125] ... Execute the command below to start the main analysis of Vulture. cd vulture/nextflow nextflow run scvh_docker_local.nf -profile dockerlocal -params-file params.yaml --outdir=your_output_directory -with-report nextflow_report_$(date +%s).html -bg &>> nextflow_log_$(date +%s).log A successful run will generate the following files in the output directory: ... nextflow_report_1628188800.html nextflow_log_1628188800.log ... The \"nextflow_report_1628188800.html\" file is a report of the analysis. The \"nextflow_log_1628188800.log\" file is the log file of the analysis. A successful run will also generate the following files in the nextflow_log_1628188800.log: N E X T F L O W ~ version 21.10.6 Launching `scvh_docker_local.nf` [cheeky_brown] - revision: 59a6446081 S C V H - N F P I P E L I N E =================================== transcriptome: /mnt/d/scvh_files/vmh_genome_dir/references reads : [SRR12570125] outdir : /mnt/d/output/ database: : viruSITE.NCBIprokaryotes threads : 10 ram : 128 alignment : STAR whitelist : 3M-february-2018.txt soloCBlen : 16 soloCBstart : 1 soloUMIstart : 17 soloUMIlen : 12 soloStrand : Forward soloMultiMappers: EM soloFeature : GeneFull outSAMtype : BAM SortedByCoordinate technology : 10XV3 pseudoBAM : inputformat : fastq sampleSubfix1 : _1 sampleSubfix2 : _2 [SRR12570125, /mnt/d/scvh_files/EXAMPLES/SRR12570125_1.fastq.gz, /mnt/d/scvh_files/EXAMPLES/SRR12570125_2.fastq.gz] [88/9dd405] Submitted process > Map (1) [Previous Step](https://juychen.github.io/docs/3_Nextflow/NextflowInstall.html){: .btn } [Next Step](https://juychen.github.io/docs/6_Local/Localusage.html){: .btn .btn-purple }","title":"Specify the configuration file for an analysis"},{"location":"2.%20Running%20on%20AWS%20Cloud/1_Setup/","text":"To get started with Vulture, you will need a AWS account and setup the environment in your account. Vulture is composed of a stack of AWS services as below: Setup This chapter guides you to configure the following AWS services which you will use in Vulture. - AWS IAM - AWS Batch - AWS S3 Create an IAM role for your Workspace Head over to the IAM console , find and click \"Create role\" button (2) under the Roles (1) section. Select the \"AWS service\" (3) and choose the \"Batch\" use case (4) hit Next button (5) at the bottom. Select the following policies and click \"attach policies\" to add them (6) . AdministratorAccess AmazonEC2FullAccess AmazonEC2ContainerRegistryFullAccess AmazonS3FullAccess AWSBatchServiceRole AdministratorAccess AWSBatchFullAccess AmazonEC2ContainerServiceforEC2Role AmazonEC2ContainerServiceRole Name the role as \"vulture-iam-role\" and click \"Create role\". Therefore, a role named \"aws-workshop-admin\" is ready for use.","title":"Setup"},{"location":"2.%20Running%20on%20AWS%20Cloud/1_Setup/#setup","text":"This chapter guides you to configure the following AWS services which you will use in Vulture. - AWS IAM - AWS Batch - AWS S3","title":"Setup"},{"location":"2.%20Running%20on%20AWS%20Cloud/1_Setup/#create-an-iam-role-for-your-workspace","text":"Head over to the IAM console , find and click \"Create role\" button (2) under the Roles (1) section. Select the \"AWS service\" (3) and choose the \"Batch\" use case (4) hit Next button (5) at the bottom. Select the following policies and click \"attach policies\" to add them (6) . AdministratorAccess AmazonEC2FullAccess AmazonEC2ContainerRegistryFullAccess AmazonS3FullAccess AWSBatchServiceRole AdministratorAccess AWSBatchFullAccess AmazonEC2ContainerServiceforEC2Role AmazonEC2ContainerServiceRole Name the role as \"vulture-iam-role\" and click \"Create role\". Therefore, a role named \"aws-workshop-admin\" is ready for use.","title":"Create an IAM role for your Workspace"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/","text":"Overview Nextflow Nextflow is free open-source software distributed under the Apache 2.0 license developed by Seqera Labs. The software is used by scientists and engineers to write, deploy and share data-intensive, highly scalable, workflows on any infrastructure. It helps us to define workflows to run the Vulture pipeline composed of multiple programs. The reason we build Vulture within Nextflow is that it can be applied to workflows in containers or on a cloud computing platform. You can refer to the Nextflow documentation for more information about Nextflow syntax and usage. Nextflow This section introduces the installation of Nextflow and its dependencies. Install Nextflow Nextflow can be installed by following the instructions at Nextflow Installation Guide . If you are using OS X/Linux, you can run the commands below to install it immediately. wget -qO- https://get.nextflow.io | bash # run from current directory chmod +x nextflow # run from anywhere sudo mv nextflow /usr/local/bin/ Install Nextflow dependencies for visualization Nextflow is able to render graphs for which it needs graphviz to be installed. jq will help us deal with JSON files. sudo yum install -y graphviz jq AWS Region Even though we are depending on an IAM Role and not local permissions some tools depend on having the AWS_REGION defined as an environment variable - let's add it to our login shell configuration. export AWS_REGION=us-east-2 echo \"AWS_REGION=${AWS_REGION}\" |tee -a ~/.bashrc","title":"Nextflow"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#overview","text":"","title":"Overview"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#nextflow","text":"Nextflow is free open-source software distributed under the Apache 2.0 license developed by Seqera Labs. The software is used by scientists and engineers to write, deploy and share data-intensive, highly scalable, workflows on any infrastructure. It helps us to define workflows to run the Vulture pipeline composed of multiple programs. The reason we build Vulture within Nextflow is that it can be applied to workflows in containers or on a cloud computing platform. You can refer to the Nextflow documentation for more information about Nextflow syntax and usage.","title":"Nextflow"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#nextflow_1","text":"This section introduces the installation of Nextflow and its dependencies.","title":"Nextflow"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#install-nextflow","text":"Nextflow can be installed by following the instructions at Nextflow Installation Guide . If you are using OS X/Linux, you can run the commands below to install it immediately. wget -qO- https://get.nextflow.io | bash # run from current directory chmod +x nextflow # run from anywhere sudo mv nextflow /usr/local/bin/","title":"Install Nextflow"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#install-nextflow-dependencies-for-visualization","text":"Nextflow is able to render graphs for which it needs graphviz to be installed. jq will help us deal with JSON files. sudo yum install -y graphviz jq","title":"Install Nextflow dependencies for visualization"},{"location":"2.%20Running%20on%20AWS%20Cloud/2_Nextflow/#aws-region","text":"Even though we are depending on an IAM Role and not local permissions some tools depend on having the AWS_REGION defined as an environment variable - let's add it to our login shell configuration. export AWS_REGION=us-east-2 echo \"AWS_REGION=${AWS_REGION}\" |tee -a ~/.bashrc","title":"AWS Region"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/","text":"Overview AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of computational resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. With AWS Batch, there is no need to install and manage batch computing software or server clusters that you use to run your jobs, allowing you to focus on analyzing results and solving problems. AWS Batch plans, schedules, and executes your batch computing workloads across the full range of AWS compute services and features, such as AWS Fargate, Amazon EC2, and Spot Instances. There is no additional charge for AWS Batch. You only pay for the AWS resources (e.g. EC2 instances) you create to store and run your batch jobs. In our tutorial, the pipeline of Nextflow and Batch deal with jobs by the following workflow automatically\uff1a We configure and run Nextflow script to trigger a submission to the job queue. The AWS Batch scheduler runs periodically to check for jobs in the job queue Once jobs are placed, the scheduler evaluates the Compute Environments and adjusts the compute capacity to allow the jobs to run. The job (packaged by the container) will run in the computational environment created. The job output will be transferred and stored in the S3 bucket. Batch Enviroment 0. Create a launch template Before you create an AWS Batch Compute Environment, please kindly refer to how to create Vulture launch template to create a launch template first. 1. Setup Batch Vulture Compute Environment 1 Create an AWS Batch Environment at AWS Batch Home . On the AWS Batch dashboard: 1) Select \"Compute environments\" on the left panel 2) Select \"Create\" to create a new compute environments In the Compute Environments page, set your preference of the compute environment as follows: Step 1 Compute environment configuration 3) Select \"Managed\" in the compute environment orchestration type 4) Change the Name in the compute environment name. We suggest that you should use \"scvh-CE-r5a4x\". 5) Select \"vulture-iam-role\" in the Service role ( IAM role settings ). 6) Select \"ecsInstanceRole\" in the Instance role. Step 2 Instance configuration 7) Select \"Use EC2 Spot Instances\" for cost-saving 8) Set \"0\" in both Minium and Desired vCPU settings, and 256 in Maximum CPUs -required 9) Scoll down to the \"Additional configuration\". 10) Select \"optimal\" in the Allowed instance type. 11) Select \"SPOT_CAPACITY_OPTIMIZED\" in the allocation strategy 12) Select \"vulture-launch-template\" under Launch template. For how to create this template beforehand. Please refer to launch template . We apply this template to every instance Batch schedules for us because we have mentioned in the previous section that the default storage of EC2 instance is not enough for the Vulture pipeline, this template will provide extra spaces for our Batch job's instance. Step 3 Network configuration 13) Select \"Default for VPC\" under the Virtual Private Cloud (VPC) ID, this will select the default VPCs . Note that all AWS accounts comes with a default VPC for use in each Region. A default VPC comes with a public subnet in each Availability Zone, an internet gateway, and settings to enable DNS resolution. Therefore, you can immediately start launching Amazon EC2 instances into a default VPC. A default VPC is suitable for getting started quickly. Leave other settings of the environment as default and create an environment. 2. Setup Batch Vulture Compute Environment 2 This basically follows the same steps as to create Compute Environment 1 above, but with different name and instance type, to enable massively parallel processing of Vulture tasks with Batch. 1) Select \"Compute environments\" on the left panel 2) Select \"Create\" to create a new compute environments Repeat the configuration steps from Step 2. to Step 13 to finish create Compute Environment 2 used for creating Compute Environment 1 above. Monitoring Batch Job Heading back to the Batch Home dashboard , you can see the overview of the number of tasks running in the job queue. To monitor your running jobs, you can click the number below the label \"RUNNING\". Here, you can find the job submited previously. Click the job name to go to the job information page. Inside the information page, you can click the link below the \"Log stream name\" label. This is where the output of the program stored. It will record any information output within the stand output stream of the console. Also, the console output will refresh continuously along with your program process. 0. View results You can select go the S3 Bucket . Looking into the path: \"s3://${BUCKET_NAME_RESULTS}/batchD/{SAMPLE_ID}/alignment_outs/\" page. Click to open it. The Vulture results should look like in the picture below: 1. View pricing of the program To check the bill of our previous run, we can go to the EC2 Dashboard and find the Spot Request section at https://console.aws.amazon.com/ec2sp/v2/home#/spot . Click saving summary to view the cost we saved by applying spot instances. This saving summary page shows the price. We know that to align a 12GB scRNA-Seq sample takes $0.17. Thanks to the spot instances, we can save 79% of money compared to hosting an on-demand EC2 instance. Batch Queue 0. Setup Batch Vulture Job Queue 1 We can return back to the home page of Batch at: https://us-east-2.console.aws.amazon.com/batch/home . 1) Select \"Job queues\" on the left panel 2) Select \"Create\" to create a new job queue 3) Name your job queue to according to your preference. 4) Select your compute environment to which you created in the previous chapter i.e. Batch Environment 5) Select \"Create\" to create a new job queue 1. Setup Batch Vulture Job Queue 2 We can return back to the home page of Batch . Create a Job Queue 2 for parallel processing of Vulture tasks by repeating the same steps from Step 1 to Step 4 used to create Job Queue 1 above.","title":"Batch"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#overview","text":"AWS Batch enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of computational resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. With AWS Batch, there is no need to install and manage batch computing software or server clusters that you use to run your jobs, allowing you to focus on analyzing results and solving problems. AWS Batch plans, schedules, and executes your batch computing workloads across the full range of AWS compute services and features, such as AWS Fargate, Amazon EC2, and Spot Instances. There is no additional charge for AWS Batch. You only pay for the AWS resources (e.g. EC2 instances) you create to store and run your batch jobs. In our tutorial, the pipeline of Nextflow and Batch deal with jobs by the following workflow automatically\uff1a We configure and run Nextflow script to trigger a submission to the job queue. The AWS Batch scheduler runs periodically to check for jobs in the job queue Once jobs are placed, the scheduler evaluates the Compute Environments and adjusts the compute capacity to allow the jobs to run. The job (packaged by the container) will run in the computational environment created. The job output will be transferred and stored in the S3 bucket.","title":"Overview"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#batch-enviroment","text":"","title":"Batch Enviroment"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#0-create-a-launch-template","text":"Before you create an AWS Batch Compute Environment, please kindly refer to how to create Vulture launch template to create a launch template first.","title":"0. Create a launch template"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#1-setup-batch-vulture-compute-environment-1","text":"Create an AWS Batch Environment at AWS Batch Home . On the AWS Batch dashboard: 1) Select \"Compute environments\" on the left panel 2) Select \"Create\" to create a new compute environments In the Compute Environments page, set your preference of the compute environment as follows: Step 1 Compute environment configuration 3) Select \"Managed\" in the compute environment orchestration type 4) Change the Name in the compute environment name. We suggest that you should use \"scvh-CE-r5a4x\". 5) Select \"vulture-iam-role\" in the Service role ( IAM role settings ). 6) Select \"ecsInstanceRole\" in the Instance role. Step 2 Instance configuration 7) Select \"Use EC2 Spot Instances\" for cost-saving 8) Set \"0\" in both Minium and Desired vCPU settings, and 256 in Maximum CPUs -required 9) Scoll down to the \"Additional configuration\". 10) Select \"optimal\" in the Allowed instance type. 11) Select \"SPOT_CAPACITY_OPTIMIZED\" in the allocation strategy 12) Select \"vulture-launch-template\" under Launch template. For how to create this template beforehand. Please refer to launch template . We apply this template to every instance Batch schedules for us because we have mentioned in the previous section that the default storage of EC2 instance is not enough for the Vulture pipeline, this template will provide extra spaces for our Batch job's instance. Step 3 Network configuration 13) Select \"Default for VPC\" under the Virtual Private Cloud (VPC) ID, this will select the default VPCs . Note that all AWS accounts comes with a default VPC for use in each Region. A default VPC comes with a public subnet in each Availability Zone, an internet gateway, and settings to enable DNS resolution. Therefore, you can immediately start launching Amazon EC2 instances into a default VPC. A default VPC is suitable for getting started quickly. Leave other settings of the environment as default and create an environment.","title":"1. Setup Batch Vulture Compute Environment 1"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#2-setup-batch-vulture-compute-environment-2","text":"This basically follows the same steps as to create Compute Environment 1 above, but with different name and instance type, to enable massively parallel processing of Vulture tasks with Batch. 1) Select \"Compute environments\" on the left panel 2) Select \"Create\" to create a new compute environments Repeat the configuration steps from Step 2. to Step 13 to finish create Compute Environment 2 used for creating Compute Environment 1 above.","title":"2. Setup Batch Vulture Compute Environment 2"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#monitoring-batch-job","text":"Heading back to the Batch Home dashboard , you can see the overview of the number of tasks running in the job queue. To monitor your running jobs, you can click the number below the label \"RUNNING\". Here, you can find the job submited previously. Click the job name to go to the job information page. Inside the information page, you can click the link below the \"Log stream name\" label. This is where the output of the program stored. It will record any information output within the stand output stream of the console. Also, the console output will refresh continuously along with your program process.","title":"Monitoring Batch Job"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#0-view-results","text":"You can select go the S3 Bucket . Looking into the path: \"s3://${BUCKET_NAME_RESULTS}/batchD/{SAMPLE_ID}/alignment_outs/\" page. Click to open it. The Vulture results should look like in the picture below:","title":"0. View results"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#1-view-pricing-of-the-program","text":"To check the bill of our previous run, we can go to the EC2 Dashboard and find the Spot Request section at https://console.aws.amazon.com/ec2sp/v2/home#/spot . Click saving summary to view the cost we saved by applying spot instances. This saving summary page shows the price. We know that to align a 12GB scRNA-Seq sample takes $0.17. Thanks to the spot instances, we can save 79% of money compared to hosting an on-demand EC2 instance.","title":"1. View pricing of the program"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#batch-queue","text":"","title":"Batch Queue"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#0-setup-batch-vulture-job-queue-1","text":"We can return back to the home page of Batch at: https://us-east-2.console.aws.amazon.com/batch/home . 1) Select \"Job queues\" on the left panel 2) Select \"Create\" to create a new job queue 3) Name your job queue to according to your preference. 4) Select your compute environment to which you created in the previous chapter i.e. Batch Environment 5) Select \"Create\" to create a new job queue","title":"0. Setup Batch Vulture Job Queue 1"},{"location":"2.%20Running%20on%20AWS%20Cloud/3_Batch/#1-setup-batch-vulture-job-queue-2","text":"We can return back to the home page of Batch . Create a Job Queue 2 for parallel processing of Vulture tasks by repeating the same steps from Step 1 to Step 4 used to create Job Queue 1 above.","title":"1. Setup Batch Vulture Job Queue 2"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/","text":"Nextflow and scRNA-Seq processing Setup the AWS CLI Install the AWS CLI and prepare the access key and secret access key ahead by following instructions at Obtaining AWS Credentials . pip install awscli aws configure # here will prompt you to enter access key and secret access key Clone Vulture source code Clone the source code of Vulture into your local computer git clone https://github.com/holab-hku/Vulture.git # change into directory below cd Vulture/nextflow # checkout the branch below for most updated code git checkout cloud-new-junyi Create S3 Bucket to store results # specify bucket names and save them into bash environment variables export BUCKET_NAME_TEMP=vulture-temp export BUCKET_NAME_RESULTS=vulture-results echo \"BUCKET_NAME_TEMP=${BUCKET_NAME_TEMP}\" |tee -a ~/.bashrc echo \"BUCKET_NAME_RESULTS=${BUCKET_NAME_RESULTS}\" |tee -a ~/.bashrc # create S3 buckets with specified bucket names aws --region ${AWS_REGION} s3 mb s3://${BUCKET_NAME_TEMP} aws --region ${AWS_REGION} s3 mb s3://${BUCKET_NAME_RESULTS} Run Vulture pipeline - 1. Build Genome reference Now we are about to run the first step of the Vulture pipeline i.e. mkref (genome reference making), execute the command below in your favourite terminal or powershell and wait it to be finished nextflow run scvh_mkref.nf -profile mkref -bucket-dir s3://${BUCKET_NAME_TEMP} --outdir=s3://${BUCKET_NAME_RESULTS}/batchA -with-report mkref_$(date +%s).html -bg &>> mkref_$(date +%s).log; The input data required for mkref stage are available in the downloadable links below, you can save them into your own S3 bucket folder: hg38.fa hg38.unique_gene_names.gtf prokaryotes.csv viruSITE_human_host.txt Also you can generate the files yourself following instructions below: VirusSITE (viruSITE human host) Click \"Format: CSV\" NCBI Prokaryotes Filters -> Host (Homo sapiens) -> Assembly level (Complete) -> RefSeq category (representative) -> Download [prokaryotes.csv] After the mkref job is done, you need to edit line in \"nextflow/nextflow.config\" file -> \"params.ref\" to the actual S3 path where your output reference genome files are i.e. in \"s3://${BUCKET_NAME_RESULTS}/batchA\" or you could download from the downloadable links below and store them into your own S3 bucket folder: human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.removed_amb_viral_exon.gtf human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.fa human_host_viruses.viruSITE.with_hg38.removed_amb_viral_exon.gtf human_host_viruses.viruSITE.with_hg38.fa ... mkref { aws.region = 'us-east-2' process.container = 'public.ecr.aws/b6a4h2a6/scvh_mkref:latest' process.executor = 'awsbatch' process.queue = 'vulture-stdq' # this line needs to be changed params.ref = 's3://vulture-reference/humangenome/' } ... Run Vulture pipeline - 2. Start main analysis Before we start our analysis, we need to edit \"nextflow/params.yaml\" file to include the reads of your interest. Here is a snippet of how the \"params.yaml\" file looks like ... soloStrand: \"Forward\" alignment: \"STAR\" technology: \"10XV2\" virus_database: \"viruSITE\" soloMultiMappers: \"EM\" soloFeatures: \"GeneFull\" inputformat: \"bam\" soloInputSAMattrBarcodeSeq: \"CB UB\" barcodes_whitelist: \"None\" reads: - \"SRR6885502\" - \"SRR6885503\" - \"SRR6885504\" - \"SRR6885505\" - \"SRR6885506\" - \"SRR6885507\" - \"SRR6885508\" ... Execute the command below to start the main analysis of Vulture. nextflow run scvh_full.nf -profile batchfull -params-file params.yaml -bucket-dir s3://${BUCKET_NAME_TEMP} --outdir=s3://${BUCKET_NAME_RESULTS}/batchD -with-report report_bam_$(date +%s).html -bg &>> submitnf_bam_$(date +%s).log","title":"Running Vulture on AWS cloud with Nextflow"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#nextflow-and-scrna-seq-processing","text":"","title":"Nextflow and scRNA-Seq processing"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#setup-the-aws-cli","text":"Install the AWS CLI and prepare the access key and secret access key ahead by following instructions at Obtaining AWS Credentials . pip install awscli aws configure # here will prompt you to enter access key and secret access key","title":"Setup the AWS CLI"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#clone-vulture-source-code","text":"Clone the source code of Vulture into your local computer git clone https://github.com/holab-hku/Vulture.git # change into directory below cd Vulture/nextflow # checkout the branch below for most updated code git checkout cloud-new-junyi","title":"Clone Vulture source code"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#create-s3-bucket-to-store-results","text":"# specify bucket names and save them into bash environment variables export BUCKET_NAME_TEMP=vulture-temp export BUCKET_NAME_RESULTS=vulture-results echo \"BUCKET_NAME_TEMP=${BUCKET_NAME_TEMP}\" |tee -a ~/.bashrc echo \"BUCKET_NAME_RESULTS=${BUCKET_NAME_RESULTS}\" |tee -a ~/.bashrc # create S3 buckets with specified bucket names aws --region ${AWS_REGION} s3 mb s3://${BUCKET_NAME_TEMP} aws --region ${AWS_REGION} s3 mb s3://${BUCKET_NAME_RESULTS}","title":"Create S3 Bucket to store results"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#run-vulture-pipeline-1-build-genome-reference","text":"Now we are about to run the first step of the Vulture pipeline i.e. mkref (genome reference making), execute the command below in your favourite terminal or powershell and wait it to be finished nextflow run scvh_mkref.nf -profile mkref -bucket-dir s3://${BUCKET_NAME_TEMP} --outdir=s3://${BUCKET_NAME_RESULTS}/batchA -with-report mkref_$(date +%s).html -bg &>> mkref_$(date +%s).log; The input data required for mkref stage are available in the downloadable links below, you can save them into your own S3 bucket folder: hg38.fa hg38.unique_gene_names.gtf prokaryotes.csv viruSITE_human_host.txt Also you can generate the files yourself following instructions below: VirusSITE (viruSITE human host) Click \"Format: CSV\" NCBI Prokaryotes Filters -> Host (Homo sapiens) -> Assembly level (Complete) -> RefSeq category (representative) -> Download [prokaryotes.csv] After the mkref job is done, you need to edit line in \"nextflow/nextflow.config\" file -> \"params.ref\" to the actual S3 path where your output reference genome files are i.e. in \"s3://${BUCKET_NAME_RESULTS}/batchA\" or you could download from the downloadable links below and store them into your own S3 bucket folder: human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.removed_amb_viral_exon.gtf human_host_viruses_microbes.viruSITE.NCBIprokaryotes.with_hg38.fa human_host_viruses.viruSITE.with_hg38.removed_amb_viral_exon.gtf human_host_viruses.viruSITE.with_hg38.fa ... mkref { aws.region = 'us-east-2' process.container = 'public.ecr.aws/b6a4h2a6/scvh_mkref:latest' process.executor = 'awsbatch' process.queue = 'vulture-stdq' # this line needs to be changed params.ref = 's3://vulture-reference/humangenome/' } ...","title":"Run Vulture pipeline - 1. Build Genome reference"},{"location":"2.%20Running%20on%20AWS%20Cloud/4_RunVulture/#run-vulture-pipeline-2-start-main-analysis","text":"Before we start our analysis, we need to edit \"nextflow/params.yaml\" file to include the reads of your interest. Here is a snippet of how the \"params.yaml\" file looks like ... soloStrand: \"Forward\" alignment: \"STAR\" technology: \"10XV2\" virus_database: \"viruSITE\" soloMultiMappers: \"EM\" soloFeatures: \"GeneFull\" inputformat: \"bam\" soloInputSAMattrBarcodeSeq: \"CB UB\" barcodes_whitelist: \"None\" reads: - \"SRR6885502\" - \"SRR6885503\" - \"SRR6885504\" - \"SRR6885505\" - \"SRR6885506\" - \"SRR6885507\" - \"SRR6885508\" ... Execute the command below to start the main analysis of Vulture. nextflow run scvh_full.nf -profile batchfull -params-file params.yaml -bucket-dir s3://${BUCKET_NAME_TEMP} --outdir=s3://${BUCKET_NAME_RESULTS}/batchD -with-report report_bam_$(date +%s).html -bg &>> submitnf_bam_$(date +%s).log","title":"Run Vulture pipeline - 2. Start main analysis"},{"location":"3.%20Building%20your%20reference%20genome/1_Build/","text":"layout: default title: Build our own reference genome nav_order: 1","title":"1 Build"},{"location":"Supplementary/Launchtemp/","text":"Setup Launch Template Before we start, we need to setup a launch template of EC2 instance for our computing environments for AWS batch. Because we have mentioned in the previous section that the storage of EC2 instance is not enough for the scRNA-Seq data preprocessing. The launch template can be created in either GUI-way or through AWS Command Line (AWS CLI). To create the template in GUI-way, navigate to AWS EC2 dashboard at AWS EC2 Home . In the left panel menu, select \"Launch Templates\" under Instances. Alternatively, you can run the following command to create the launch template from CLI. The launch template file in our project (launch-template-data.json) is shown as follows: { \"LaunchTemplateName\": \"increase-volume\", \"LaunchTemplateData\": { \"BlockDeviceMappings\": [ { \"DeviceName\": \"/dev/xvda\", \"Ebs\": { \"VolumeSize\": 2500, \"VolumeType\": \"gp2\" } } ] } } With this template name \"vulture-launch-template\", we can increase the default storage of EC2 instances to 2500GB. We can run the following script: Run the command below to create this template. aws ec2 --region ${AWS_REGION} create-launch-template --cli-input-json launch-template-data.json","title":"Launch template Setup"},{"location":"Supplementary/Launchtemp/#setup-launch-template","text":"Before we start, we need to setup a launch template of EC2 instance for our computing environments for AWS batch. Because we have mentioned in the previous section that the storage of EC2 instance is not enough for the scRNA-Seq data preprocessing. The launch template can be created in either GUI-way or through AWS Command Line (AWS CLI). To create the template in GUI-way, navigate to AWS EC2 dashboard at AWS EC2 Home . In the left panel menu, select \"Launch Templates\" under Instances. Alternatively, you can run the following command to create the launch template from CLI. The launch template file in our project (launch-template-data.json) is shown as follows: { \"LaunchTemplateName\": \"increase-volume\", \"LaunchTemplateData\": { \"BlockDeviceMappings\": [ { \"DeviceName\": \"/dev/xvda\", \"Ebs\": { \"VolumeSize\": 2500, \"VolumeType\": \"gp2\" } } ] } } With this template name \"vulture-launch-template\", we can increase the default storage of EC2 instances to 2500GB. We can run the following script: Run the command below to create this template. aws ec2 --region ${AWS_REGION} create-launch-template --cli-input-json launch-template-data.json","title":"Setup Launch Template"},{"location":"Supplementary/Supplementary/","text":"A typical AWS Batch workload might be triggered by input data being uploaded to S3, this will kick in multiple stages. Overview The Upload event triggers a submission to a job queue (e.g. via lambda function) The AWS Batch scheduler runs periodically to check for jobs in the job queue Once jobs are placed, the scheduler evaluates the Compute Environments and adjusts the compute capacity to allow the jobs to run. The job output is stored external to the job (e.g. S3, EFS or FSx for Lustre)","title":"Supplementary"},{"location":"Supplementary/Supplementary/#overview","text":"The Upload event triggers a submission to a job queue (e.g. via lambda function) The AWS Batch scheduler runs periodically to check for jobs in the job queue Once jobs are placed, the scheduler evaluates the Compute Environments and adjusts the compute capacity to allow the jobs to run. The job output is stored external to the job (e.g. S3, EFS or FSx for Lustre)","title":"Overview"}]}